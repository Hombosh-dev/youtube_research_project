---
title: "youtube_research_project"
author: "Kosiv Marta, Hombosh Oleh, Kolodchak Bohdan"
date: "`r Sys.Date()`"
output: html_document
---

## Project Description

##### Data Source: Kaggle: [Kaggle: Trending YouTube Video Statistics](https://www.kaggle.com/datasets/datasnaek/youtube-new)

For our research project, we have selected the Trending YouTube Video Statistics dataset. This dataset contains information about the daily trending YouTube videos in several countries (US, GB, DE, CA). It provides variables such as: views, likes, dislikes, comment_count, tags, category_id.

This data allows us to analyze what makes a video popular and test different hypotheses about viewer engagement.

Some useful packages:

```{r}
library(tidyverse)
library(readr)
```

The first step was to read data and look what actually we can analyze.

```{r}
path <- "dataset/"
files <- list.files(path, pattern = "*.csv", full.names = TRUE)

df_list <- lapply(files, function(file) {
  data <- read_csv(file, show_col_types = FALSE)
  
  country_name <- file %>%
    basename() %>%
    substr(1, 2)
  
  data$country <- country_name
  return(data)
})

df <- bind_rows(df_list)

print(head(df))
```


## Part I. Pareto Principle (80/20)

We analyze the inequality of channel views to test if they follow a **Power-Law (Pareto) distribution**.

### 1. Theory & Parameter Estimation (MLE)

#### Mathematical Derivation of MLE Estimator

To justify our calculation of $\alpha$, we derive the Maximum Likelihood Estimator (MLE) from the Pareto Probability Density Function (PDF).

**1. The Likelihood Function** Given a sample of observations $x_1, x_2, ..., x_n$ following a Pareto distribution with PDF $f(x) = \frac{\alpha x_m^\alpha}{x^{\alpha+1}}$, the likelihood function $L(\alpha)$ is the product of individual probabilities:

$$L(\alpha) = \prod_{i=1}^n \frac{\alpha x_m^\alpha}{x_i^{\alpha+1}} = \alpha^n x_m^{n\alpha} \prod_{i=1}^n x_i^{-(\alpha+1)}$$

**2. The Log-Likelihood Function** It is computationally easier to maximize the log-likelihood ($\ln L$). Taking the natural logarithm transforms the product into a sum:

$$\ln L(\alpha) = n \ln \alpha + n\alpha \ln x_m - (\alpha+1) \sum_{i=1}^n \ln x_i$$

**3. Maximization** To find the optimal $\alpha$, we take the derivative with respect to $\alpha$ and set it to zero:

$$\frac{\partial \ln L}{\partial \alpha} = \frac{n}{\alpha} + n \ln x_m - \sum_{i=1}^n \ln x_i = 0$$

**4. Solving for** $\alpha$ Rearranging the terms allows us to isolate $\hat{\alpha}$:

$$\frac{n}{\hat{\alpha}} = \sum_{i=1}^n \ln x_i - n \ln x_m = \sum_{i=1}^n (\ln x_i - \ln x_m)$$ $$\frac{n}{\hat{\alpha}} = \sum_{i=1}^n \ln \left( \frac{x_i}{x_m} \right)$$

Finally, we obtain the formula used in our code:

$$\hat{\alpha} = \frac{n}{\sum_{i=1}^n \ln(x_i / x_m)}$$

where $x_m$ is the minimum value and $\alpha$ is the shape parameter (tail index).

Below, we perform this calculation manually in R.

### 2. Data Processing & Visualization

#### Data processing

First, we aggregate the data to handle duplicates. Since the dataset contains daily snapshots, the same video appears multiple times. To avoid double-counting, we extract the **peak (maximum) views** for each unique video before summing them up for the channel.

```{r}
channel_stats <- df %>%
  group_by(video_id) %>%
  summarise(
    channel_title = first(channel_title),
    max_video_views = max(views, na.rm = TRUE), # Peak views for the video
    .groups = "drop"
  ) %>%
  group_by(channel_title) %>%
  summarise(total_views = sum(max_video_views, na.rm = TRUE)) %>%
  filter(total_views > 0) %>%
  arrange(desc(total_views))

x_min <- 5000 
pareto_data <- channel_stats %>% filter(total_views >= x_min)

n <- nrow(pareto_data)

log_sum <- sum(log(pareto_data$total_views / x_min))
alpha_hat <- n / log_sum

print(paste("Sample size (tail n):", n))
print(paste("Minimum views (x_min):", x_min))
print(paste("Estimated Alpha (Shape Parameter):", round(alpha_hat, 4)))
```

The calculated shape parameter $\alpha \approx 0.28$ indicates an extremely heavy tail. In standard Pareto distributions, $\alpha$ typically ranges between 1 and 3. A value below 1 implies that the mathematical mean of the distribution is infinite. This confirms that the YouTube ecosystem is dominated by "black swan" events-viral videos that skew all average metrics.

#### Figure 1: Distribution of Views (Linear Scale)

As described in the problem statement, we first observe the distribution on a standard linear scale to identify skewness.

```{r}
ggplot(channel_stats, aes(x = total_views)) +
  geom_histogram(bins = 50, fill = "steelblue", color = "white") +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Figure 1: Distribution of Total Views (Linear Scale)",
       subtitle = "Extreme positive skewness suggests a heavy-tailed distribution",
       x = "Total Views", y = "Count of Channels") +
  theme_minimal()
```

#### Figure 2: Log-Log Scale Analysis

To confirm the Pareto law, we plot the tail data ($x \ge x_{min}$) on a log-log scale. A straight line indicates adherence to the power law.

```{r}
ggplot(pareto_data, aes(x = total_views)) +
  geom_histogram(bins = 50, fill = "#69b3a2", color = "white", alpha = 0.8) +
  scale_x_log10(labels = scales::comma, 
                breaks = scales::trans_breaks("log10", function(x) 10^x)) + 
  scale_y_log10(breaks = scales::trans_breaks("log10", function(x) 10^x)) +
  labs(title = "Figure 2: Distribution of Views (Log-Log Scale)",
       subtitle = paste("Tail analysis (x >= ", x_min, ") shows linear descent"),
       x = "Total Views (Log Scale)", 
       y = "Count of Channels (Log Scale)") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"))
```

Figure 1 confirms the extreme positive skewness of the data. The vast majority of channels cluster near zero, while a tiny fraction extends far to the right, making the tail invisible on a standard linear scale.To test the Power-Law hypothesis, we switch to a log-log scale (Figure 2).

A signature feature of Pareto distributions is a straight line on this plot. As observed, the filtered tail ($x \ge 5000$) follows a clear linear descent, providing strong visual evidence for the Power-Law model.

### 3. Inequality Visualization (Lorenz Curve)

To quantify the concentration of views, we construct a Lorenz curve. If views were distributed equally, the curve would follow the diagonal ($y=x$).

```{r}
alpha_8020 <- log(5) / log(4)

lorenz_data <- pareto_data %>%
  arrange(total_views) %>%
  mutate(
    cum_channels = row_number() / n(),
    cum_views = cumsum(total_views) / sum(total_views),
    theoretical_8020 = 1 - (1 - cum_channels)^(1 - 1/alpha_8020)
  )

top_20_share <- lorenz_data %>% 
  filter(cum_channels >= 0.8) %>% 
  head(1) %>% 
  pull(cum_views)

ggplot(lorenz_data, aes(x = cum_channels)) +
  geom_abline(intercept = 0, slope = 1, linetype = "dotted", color = "gray", size = 1) +
  annotate("text", x = 0.3, y = 0.35, label = "Perfect Equality", color = "gray", angle = 45) +

  geom_line(aes(y = theoretical_8020), color = "steelblue", linetype = "dashed", size = 1) +
  annotate("text", x = 0.5, y = 0.18, label = "Classic 80/20 Rule", color = "steelblue") +

  geom_line(aes(y = cum_views), color = "darkred", size = 1.2) +
  
  annotate("text", x = 0.65, y = 0.05, 
           label = paste0("YouTube Reality:\nBottom 80% own ", round(top_20_share*100, 2), "%"), 
           color = "darkred", fontface="bold", hjust = 0) +

  labs(title = "Lorenz Curve: YouTube vs. Classic Pareto",
       subtitle = "Comparing Empirical Data against the standard 80/20 Rule",
       x = "Cumulative % of Channels", y = "Cumulative % of Views") +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()
```

Conclusion: The curve is extremely convex, staying close to the x-axis until the very end. This illustrates the "Winner-Takes-All" nature of YouTube, where the top percentile of creators captures the lion's share of attention.

### 4. Kolmogorov-Smirnov test

Finally, we formally test the goodness of fit using the Kolmogorov-Smirnov (KS) test.

Hypotheses:

$H_0$: The channel views follow a Pareto distribution.

$H_1$: The channel views do not follow a Pareto distribution.

The KS statistic $D$ measures the maximum distance between the empirical CDF and the theoretical CDF:$$D = \max_x |F_{empirical}(x) - F_{theoretical}(x)|$$

```{r}
# Define theoretical Pareto CDF function
ppareto_manual <- function(x, xm, alpha) { ifelse(x < xm, 0, 1 - (xm / x)^alpha) }

# Run KS Test on the filtered tail data
ks_res <- ks.test(jitter(pareto_data$total_views), "ppareto_manual", xm = x_min, alpha = alpha_hat)
print(ks_res)

# Visualization: Empirical vs Theoretical CDF
ggplot(pareto_data, aes(x = total_views)) +
  stat_ecdf(geom = "step", color = "black", size = 1, aes(linetype = "Empirical Data")) +
  stat_function(fun = ppareto_manual, args = list(xm = x_min, alpha = alpha_hat), 
                color = "red", size = 1, aes(linetype = "Theoretical Pareto")) +
  scale_x_log10(labels = scales::comma) +
  labs(title = "KS-Test: Empirical vs Theoretical CDF",
       subtitle = "Visual comparison of the actual data against the ideal Pareto model",
       x = "Total Views (Log Scale)", y = "Cumulative Probability") +
  scale_linetype_manual(name = "Legend", values = c("solid", "dashed"), 
                        guide = guide_legend(override.aes = list(color = c("black", "red")))) +
  theme_minimal() + theme(legend.position = "bottom")
```

### 5. Conclusion

The KS-test yields a p-value $< 2.2 \times 10^{-16}$, leading to a formal rejection of $H_0$. This is a standard outcome when applying strict statistical tests to large datasets ($N > 30,000$), where even minor deviations are flagged as significant.

However, the Log-Log visualization (Figure 2) and the ECDF plot above demonstrate that the theoretical Pareto curve (red dashed line) closely tracks the empirical data. Therefore, we conclude that the Pareto distribution is a strong descriptive model for the trending video ecosystem, confirming the extreme inequality in viewer attention.

## Part 2. The mediation of likes between comment and "go to next day in trends" probability.

We test whether likes serve as an effective mediator explaining how the number of comments influences a video’s probability of appearing in the trends on the following day.

### 1. Theory and variables for research

In this section, we examine whether the effect of the comment rate\
$$X = comments / views$$\
on the probability that a video reappears in trending the following day\
$$Y = trending\_next\_day$$\
is transmitted indirectly through the likes ratio\
$$M = likes / views.$$

The probability of appearing in trending the next day is defined as a binary event:

$$
Y_i =
\begin{cases}
1, & \text{if video } i \text{ appears again on day } t+1 \\
0, & \text{otherwise}
\end{cases}
$$

and conceptually represents\
$$
\underbrace{P(Y=1)}_{\text{probability to reappear in trending tomorrow}}.
$$

This metric captures *momentum*: a video that stays in trending is favored by YouTube’s algorithm.

#### Hypotheses

We test whether likes act as a statistical mediator:

-   **Null hypothesis**

$$H_0: a \cdot b = 0$$

-   **Alternative hypothesis**

$$H_1: a \cdot b \ne 0$$

Here, $a$ and $b$ are coefficients in the mediation pathway.

#### Model Specification

The mediation structure follows the classical framework:

1.  **Effect of comments on likes** (mediator model):

$$
M = aX + \varepsilon_M
$$

2.  **Effect of comments and likes on trending probability** (outcome model):

$$
logit(P(Y=1)) = c'X + bM + \varepsilon_Y
$$

where

$$
logit(p) = \ln \frac{p}{1 - p}
$$

transforms probabilities (bounded between 0 and 1) into real numbers so that they can be modeled as a linear combination of predictors.

3.  **Total effect of comments** (model without mediator):

$$
logit(P(Y=1)) = cX + \varepsilon.
$$

The coefficients have the following interpretation:

-   $a$: how much comments increase likes\
-   $b$: how much likes increase trending probability\
-   $c$: total effect of comments\
-   $c'$: direct effect of comments after controlling for likes

#### Evaluating the Total Effect of Comments

The causal effects are defined as:

-   **Indirect effect (IE)**:\
    $$
    IE = a \cdot b
    $$

-   **Direct effect (DE)**:\
    $$
    DE = c'
    $$

-   **Total effect (TE)**:\
    $$
    TE = c
    $$

These quantities satisfy the identity:

$$
TE = DE + IE.
$$

This equality **always holds** by construction of the regression models; it is not itself a hypothesis test.

What we test instead is:

> **How much of the total effect** $c$ is transmitted through the mediator?

If $IE$ is large and significant → likes explain part of the effect of comments.

If $IE = 0$ → likes do not mediate anything.

If both $DE$ and $IE$ are significant → partial mediation.

If $DE$ becomes small or non-significant while $IE$ is significant → full mediation.

#### Why We Test Only the Indirect Effect $a \cdot b$

The identity\
$$TE = DE + IE$$\
is always mathematically true, so we do **not** test this equality. Instead, we test whether:

$$
IE = a \cdot b \ne 0.
$$

If $IE$ is non-zero → *likes mediate the effect*.

If $IE = 0$ → *likes do not mediate anything*.

#### Testing the Indirect Effect: Bootstrap Procedure

Because the product $a \cdot b$ does not follow a normal distribution, we assess its significance using bootstrap resampling.

We repeatedly (e.g., 10,000 times):

1.  Sample videos **with replacement**
2.  Refit both regression models to obtain new estimates $a^*$ and $b^*$
3.  Compute the bootstrap indirect effect:

$$
IE^* = a^* \cdot b^*.
$$

The bootstrap-based $p$-value is

$$
p = P(|a \cdot b| \le |a^* \cdot b^*|),
$$

where the probability is taken over the bootstrap distribution.

If the bootstrap confidence interval excludes zero → **significant mediation**.

#### Summary Diagram of Effects

$$
X \longrightarrow M \longrightarrow Y
$$

with:

$$
\begin{aligned}
a &: X \rightarrow M \\
b &: M \rightarrow Y \\
c &: X \rightarrow Y \ (\text{total}) \\
c' &: X \rightarrow Y \ (\text{direct})
\end{aligned}
$$

#### Conclusion

If the indirect effect $a \cdot b$ is statistically significant, then:

> **The likes ratio partially mediates the influence of comment rate on a video's probability of reappearing in trending on the next day.**

Otherwise:

> **Comment activity and likes contribute independently to trending recurrence.**

### 2. Data processing and vizualization
add comment_rate, like ratio, trending_next_day

```{r}
# Вибираємо 5000 унікальних video_id випадковим чином
sample_videos <- sample(unique(df$video_id), 100000)

# Фільтруємо датафрейм для цих відео
df_mediation <- df %>% filter(video_id %in% sample_videos)

# Створення похідних змінних
df_mediation <- df_mediation %>%
  mutate(
    comment_rate = (comment_count / views)*1000,
    likes_ratio = (likes / views)*1000,
    trending_date = ymd_hms(publish_time, tz = "UTC")
  )

# Позначаємо, чи відео потрапило в тренди на наступний день
df_mediation <- df_mediation %>%
  arrange(video_id, trending_date) %>%
  group_by(video_id) %>%
  mutate(
    trending_next_day = lead(trending_date, 1) <= (trending_date + days(1)) &
                        lead(trending_date, 1) > trending_date
  ) %>%
  ungroup()

df_mediation$trending_next_day <- as.numeric(df_mediation$trending_next_day)

head(df_mediation)


```

Vizual - Distribution of Comment Counts
```{r}

ggplot(df_mediation, aes(x = comment_count)) +
  geom_histogram(bins = 50, fill = "skyblue", color = "black") +
  scale_x_log10() +
  labs(title = "Distribution of Comment Counts",
       x = "Comment Count (log scale)", y = "Frequency")

```
vizual Top Channels by Videos Trending Next Day + distribution of likes
```{r}
# Histogram of likes
ggplot(df_mediation, aes(x = likes)) +
  geom_histogram(bins = 50, fill = "salmon", color = "black") +
  scale_x_log10() +
  labs(title = "Distribution of Likes",
       x = "Likes (log scale)", y = "Frequency")

# Top 10 channels by number of next-day trending videos
top_channels <- df_mediation %>%
  filter(trending_next_day == 1) %>%
  group_by(channel_title) %>%
  summarise(n_videos = n()) %>%
  arrange(desc(n_videos)) %>%
  slice(1:10)

ggplot(top_channels, aes(x = reorder(channel_title, n_videos), y = n_videos)) +
  geom_bar(stat="identity", fill="purple") +
  coord_flip() +
  labs(title="Top Channels by Videos Trending Next Day",
       x="Channel", y="Number of Videos")


```
vizual comments rate vs trend in next day
```{r}

# Розмір датасету
cat("Number of rows in dataset:", nrow(df_mediation), "\n")

# Підрахунок відео за статусом наступного дня
table_trending <- table(df_mediation$trending_next_day)
table_trending

# Альтернативно, вивести у відсотках
prop.table(table_trending)


```
comments vs likes
```{r}

ggplot(df_mediation, aes(x=comment_rate, y=likes_ratio)) +
  geom_point(alpha=0.3) +
  labs(title="Likes Ratio vs Comment Rate", x="Comment Rate", y="Likes Ratio")


```



### 3. Finding needed parameters

So we created dataset where we are having for every video his trending_next_day, like_ratio, and comment_ratio.

#### linear Regression model for comments and likes
Here we want to find Mediator(likes) dependency of Comments
$$
M = aX + a_0
$$

```{r}
# Let`s find a
mean_of_comments = mean(df_mediation$comment_rate)
mean_of_likes = mean(df_mediation$likes_ratio)


a = sum((df_mediation$comment_rate - mean_of_comments)*(df_mediation$likes_ratio - mean_of_likes))/sum((df_mediation$comment_rate - mean_of_comments)^2)

a0 = mean_of_likes * 1000 - a*mean_of_comments* 1000

cat("a = ", a, "\n")
cat("a0 = ", a0 , "\n")
lm_fit <- lm(likes_ratio ~ comment_rate, data=df_mediation)

ggplot(df_mediation, aes(x = comment_rate*1000, y = likes_ratio*1000)) +
  geom_point(alpha = 0.3) +
  geom_abline(intercept = a0, slope = a, color = "red", size = 1) +
  labs(
    title = "Likes Ratio vs Comment Rate",
    x = "Comment Rate * 1000",
    y = "Likes Ratio * 1000"
  ) +
  theme_minimal()


summary(lm_fit)

```


#####logit regression for comments - likes -> trending_next_day

```{r}
# ========================================
# MLE ОЦІНКИ ДЛЯ ЛОГІСТИЧНОЇ МЕДІАЦІЇ
# ========================================

# Підготовка даних
X_original <- df_mediation$comment_rate
M_original <- df_mediation$likes_ratio
Y <- df_mediation$trending_next_day

# Видалення NA
valid_idx <- !is.na(X_original) & !is.na(M_original) & !is.na(Y)
X_original <- X_original[valid_idx]
M_original <- M_original[valid_idx]
Y <- Y[valid_idx]

cat("Кількість спостережень:", length(Y), "\n")
cat("Y=1:", sum(Y), ", Y=0:", sum(1-Y), "\n\n")

# Масштабування × 1000
X <- X_original * 1000
M <- M_original * 1000
n <- length(X)

# ========================================
# КРОК 1: M ~ X (лінійна регресія)
# ========================================

x_bar <- mean(X)
m_bar <- mean(M)
y_bar <- mean(Y)

X_tilde <- X - x_bar
M_tilde <- M - m_bar

S_xx <- sum(X_tilde^2)
S_xm <- sum(X_tilde * M_tilde)

a <- S_xm / S_xx
a0 <- m_bar - a * x_bar

cat("=== M ~ X ===\n")
cat("a =", a, "\n")
cat("a0 =", a0, "\n\n")

# ========================================
# КРОК 2: Y ~ X + M (логістична регресія IRLS)
# ========================================

# Початкові оцінки (МНК)
Y_tilde <- Y - y_bar
S_mm <- sum(M_tilde^2)
S_xy <- sum(X_tilde * Y_tilde)
S_my <- sum(M_tilde * Y_tilde)

D <- S_xx * S_mm - S_xm^2

b <- (S_xx * S_my - S_xm * S_xy) / D
c_prime <- (S_mm * S_xy - S_xm * S_my) / D
intercept_Y <- y_bar - c_prime * x_bar - b * m_bar

# IRLS ітерації
epsilon <- 1e-6
max_iter <- 50
iter <- 0

repeat {
  iter <- iter + 1
  
  c_prime_old <- c_prime
  b_old <- b
  intercept_Y_old <- intercept_Y
  
  # Предиктор та ймовірності
  eta <- c_prime * X + b * M + intercept_Y
  eta <- pmax(pmin(eta, 20), -20)
  p <- 1 / (1 + exp(-eta))
  p <- pmax(pmin(p, 1 - 1e-10), 1e-10)
  
  # Ваги та working response
  w <- p * (1 - p)
  w <- pmax(w, 1e-8)
  u <- eta + (Y - p) / w
  
  if (any(is.na(u))) {
    c_prime <- c_prime_old
    b <- b_old
    intercept_Y <- intercept_Y_old
    break
  }
  
  # Зважені середні
  w_sum <- sum(w)
  x_bar_w <- sum(w * X) / w_sum
  m_bar_w <- sum(w * M) / w_sum
  u_bar_w <- sum(w * u) / w_sum
  
  X_tilde_w <- X - x_bar_w
  M_tilde_w <- M - m_bar_w
  u_tilde_w <- u - u_bar_w
  
  # Зважені суми
  S_xx_w <- sum(w * X_tilde_w^2)
  S_mm_w <- sum(w * M_tilde_w^2)
  S_xm_w <- sum(w * X_tilde_w * M_tilde_w)
  S_xu_w <- sum(w * X_tilde_w * u_tilde_w)
  S_mu_w <- sum(w * M_tilde_w * u_tilde_w)
  
  if (any(is.na(c(S_xx_w, S_mm_w, S_xm_w, S_xu_w, S_mu_w)))) {
    c_prime <- c_prime_old
    b <- b_old
    intercept_Y <- intercept_Y_old
    break
  }
  
  D_w <- S_xx_w * S_mm_w - S_xm_w^2
  
  if (is.na(D_w) || abs(D_w) < 1e-10) {
    c_prime <- c_prime_old
    b <- b_old
    intercept_Y <- intercept_Y_old
    break
  }
  
  # Оновлені оцінки
  b <- (S_xx_w * S_mu_w - S_xm_w * S_xu_w) / D_w
  c_prime <- (S_mm_w * S_xu_w - S_xm_w * S_mu_w) / D_w
  intercept_Y <- u_bar_w - c_prime * x_bar_w - b * m_bar_w
  
  # Перевірка збіжності
  delta_b <- abs(b - b_old)
  delta_c <- abs(c_prime - c_prime_old)
  delta_int <- abs(intercept_Y - intercept_Y_old)
  
  if (is.na(delta_b) || is.na(delta_c) || is.na(delta_int)) {
    c_prime <- c_prime_old
    b <- b_old
    intercept_Y <- intercept_Y_old
    break
  }
  
  if (iter >= max_iter) break
  if (delta_b < epsilon && delta_c < epsilon && delta_int < epsilon) break
}

cat("=== Y ~ X + M (збіжність на ітерації", iter, ") ===\n")
cat("c_prime (масштабований) =", c_prime, "\n")
cat("b (масштабований) =", b, "\n")
cat("intercept_Y =", intercept_Y, "\n\n")

# У вихідному масштабі
c_prime_original <- c_prime * 1000
b_original <- b * 1000

cat("=== Оцінки у вихідному масштабі ===\n")
cat("c_prime =", c_prime_original, "\n")
cat("b =", b_original, "\n")
cat("intercept_Y =", intercept_Y, "\n\n")

# Медіаційні ефекти
IE <- (a * 1000) * (b * 1000) / 1000
DE <- c_prime * 1000


cat("=== Медіаційні ефекти ===\n")
cat("Indirect Effect (a × b) =", IE, "\n")
cat("Direct Effect (c') =", DE, "\n")

# Перевірка з glm
logit_outcome <- glm(Y ~ X_original + M_original, family = binomial(link = "logit"))
cat("=== Порівняння з glm() ===\n")
print(coef(logit_outcome))
cat("\n")

# Статистика
cat("=== Статистика прогнозів ===\n")
cat("Мін P(Y=1):", min(p_pred), "\n")
cat("Медіана P(Y=1):", median(p_pred), "\n")
cat("Середня P(Y=1):", mean(p_pred), "\n")
cat("Макс P(Y=1):", max(p_pred), "\n")
cat("Частка Y=1 у даних:", mean(Y), "\n")
```

graphs
```{r}
# ========================================
# ГРАФІКИ
# ========================================

par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# 1. M ~ X
plot(X_original, M_original, pch = 16, cex = 0.3, col = rgb(0, 0, 0, 0.2),
     xlab = "Comment Rate", ylab = "Likes Ratio", main = "Медіатор M ~ X")
abline(a = a0/1000, b = a, col = "blue", lwd = 2)
legend("topleft", legend = paste0("M = ", round(a0/1000, 3), " + ", round(a, 3), " × X"),
       col = "blue", lwd = 2, bty = "n")

# 2. Predicted probabilities (з трендом)
eta_pred <- c_prime_original * X_original + b_original * M_original + intercept_Y
p_pred <- 1 / (1 + exp(-eta_pred))

# Бінування для тренду
x_bins <- cut(X_original, breaks = 20, include.lowest = TRUE)
x_bin_mid <- tapply(X_original, x_bins, mean)
p_bin_mean <- tapply(p_pred, x_bins, mean)
y_bin_obs <- tapply(Y, x_bins, mean)

plot(X_original, p_pred, pch = 16, cex = 0.3, col = rgb(0, 0, 1, 0.15),
     xlab = "Comment Rate", ylab = "P(Y=1)", main = "Прогнозована ймовірність",
     ylim = c(0, max(p_bin_mean, y_bin_obs, na.rm = TRUE) * 1.2))
lines(x_bin_mid, p_bin_mean, col = "blue", lwd = 3)
points(x_bin_mid, y_bin_obs, col = "red", pch = 19, cex = 1.2)
legend("topleft", legend = c("Прогноз (тренд)", "Спостережувана частота"), 
       col = c("blue", "red"), lwd = c(3, NA), pch = c(NA, 19), bty = "n")

# 3. Розподіл predicted probabilities (логарифмічна шкала)
hist(log10(p_pred + 1e-10), breaks = 50, col = "lightblue", border = "white",
     xlab = "log10(P(Y=1))", main = "Розподіл ймовірностей (log-шкала)")
abline(v = log10(mean(p_pred)), col = "red", lwd = 2, lty = 2)

# 4. Калібраційний графік
p_bins <- cut(p_pred, breaks = quantile(p_pred, probs = seq(0, 1, 0.1)), include.lowest = TRUE)
obs_freq <- tapply(Y, p_bins, mean)
pred_freq <- tapply(p_pred, p_bins, mean)

plot(pred_freq, obs_freq, pch = 19, cex = 1.5, col = "darkgreen",
     xlab = "Прогнозована P(Y=1)", ylab = "Спостережувана частота Y=1", 
     main = "Калібрація моделі", xlim = c(0, max(pred_freq)*1.1), ylim = c(0, max(obs_freq, na.rm=TRUE)*1.1))
abline(0, 1, lty = 2, col = "gray", lwd = 2)
grid()

par(mfrow = c(1, 1))

```


