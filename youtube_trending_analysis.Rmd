---
title: "youtube_research_project"
author: "Kosiv Marta, Hombosh Oleh, Kolodchak Bohdan"
date: "`r Sys.Date()`"
output: html_document
---

## Project Description

##### Data Source: Kaggle: [Kaggle: Trending YouTube Video Statistics](https://www.kaggle.com/datasets/datasnaek/youtube-new)

For our research project, we have selected the Trending YouTube Video Statistics dataset. This dataset contains information about the daily trending YouTube videos in several countries (US, GB, DE, CA). It provides variables such as: views, likes, dislikes, comment_count, tags, category_id.

This data allows us to analyze what makes a video popular and test different hypotheses about viewer engagement.

Some useful packages:

```{r}
library(tidyverse)
library(readr)
library(lubridate)
library(ggplot2)
library(dplyr)
library(scales)
```

The first step was to read data and look what actually we can analyze.

```{r}
path <- "dataset/"
files <- list.files(path, pattern = "*.csv", full.names = TRUE)

df_list <- lapply(files, function(file) {
  data <- read_csv(file, show_col_types = FALSE)
  
  country_name <- file %>%
    basename() %>%
    substr(1, 2)
  
  data$country <- country_name
  return(data)
})

df <- bind_rows(df_list)

print(head(df))
```

## Part I. Pareto Principle (80/20)

We analyze the inequality of channel views to test if they follow a **Power-Law (Pareto) distribution**.

### 1. Theory & Parameter Estimation (MLE)

#### Mathematical Derivation of MLE Estimator

To justify our calculation of $\alpha$, we derive the Maximum Likelihood Estimator (MLE) from the Pareto Probability Density Function (PDF).

**1. The Likelihood Function** Given a sample of observations $x_1, x_2, ..., x_n$ following a Pareto distribution with PDF $f(x) = \frac{\alpha x_m^\alpha}{x^{\alpha+1}}$, the likelihood function $L(\alpha)$ is the product of individual probabilities:

$$L(\alpha) = \prod_{i=1}^n \frac{\alpha x_m^\alpha}{x_i^{\alpha+1}} = \alpha^n x_m^{n\alpha} \prod_{i=1}^n x_i^{-(\alpha+1)}$$

**2. The Log-Likelihood Function** It is computationally easier to maximize the log-likelihood ($\ln L$). Taking the natural logarithm transforms the product into a sum:

$$\ln L(\alpha) = n \ln \alpha + n\alpha \ln x_m - (\alpha+1) \sum_{i=1}^n \ln x_i$$

**3. Maximization** To find the optimal $\alpha$, we take the derivative with respect to $\alpha$ and set it to zero:

$$\frac{\partial \ln L}{\partial \alpha} = \frac{n}{\alpha} + n \ln x_m - \sum_{i=1}^n \ln x_i = 0$$

**4. Solving for** $\alpha$ Rearranging the terms allows us to isolate $\hat{\alpha}$:

$$\frac{n}{\hat{\alpha}} = \sum_{i=1}^n \ln x_i - n \ln x_m = \sum_{i=1}^n (\ln x_i - \ln x_m)$$ $$\frac{n}{\hat{\alpha}} = \sum_{i=1}^n \ln \left( \frac{x_i}{x_m} \right)$$

Finally, we obtain the formula used in our code:

$$\hat{\alpha} = \frac{n}{\sum_{i=1}^n \ln(x_i / x_m)}$$

where $x_m$ is the minimum value and $\alpha$ is the shape parameter (tail index).

Below, we perform this calculation manually in R.

### 2. Data Processing & Visualization

#### Data processing

First, we aggregate the data to handle duplicates. Since the dataset contains daily snapshots, the same video appears multiple times. To avoid double-counting, we extract the **peak (maximum) views** for each unique video before summing them up for the channel.

```{r}
channel_stats <- df %>%
  group_by(video_id) %>%
  summarise(
    channel_title = first(channel_title),
    max_video_views = max(views, na.rm = TRUE), # Peak views for the video
    .groups = "drop"
  ) %>%
  group_by(channel_title) %>%
  summarise(total_views = sum(max_video_views, na.rm = TRUE)) %>%
  filter(total_views > 0) %>%
  arrange(desc(total_views))

x_min <- 5000 
pareto_data <- channel_stats %>% filter(total_views >= x_min)

n <- nrow(pareto_data)

log_sum <- sum(log(pareto_data$total_views / x_min))
alpha_hat <- n / log_sum

print(paste("Sample size (tail n):", n))
print(paste("Minimum views (x_min):", x_min))
print(paste("Estimated Alpha (Shape Parameter):", round(alpha_hat, 4)))
```

The calculated shape parameter $\alpha \approx 0.28$ indicates an extremely heavy tail. In standard Pareto distributions, $\alpha$ typically ranges between 1 and 3. A value below 1 implies that the mathematical mean of the distribution is infinite. This confirms that the YouTube ecosystem is dominated by "black swan" events-viral videos that skew all average metrics.

#### Figure 1: Distribution of Views (Linear Scale)

As described in the problem statement, we first observe the distribution on a standard linear scale to identify skewness.

```{r}
ggplot(channel_stats, aes(x = total_views)) +
  geom_histogram(bins = 50, fill = "steelblue", color = "white") +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Figure 1: Distribution of Total Views (Linear Scale)",
       subtitle = "Extreme positive skewness suggests a heavy-tailed distribution",
       x = "Total Views", y = "Count of Channels") +
  theme_minimal()
```

#### Figure 2: Log-Log Scale Analysis

To confirm the Pareto law, we plot the tail data ($x \ge x_{min}$) on a log-log scale. A straight line indicates adherence to the power law.

```{r}
ggplot(pareto_data, aes(x = total_views)) +
  geom_histogram(bins = 50, fill = "#69b3a2", color = "white", alpha = 0.8) +
  scale_x_log10(labels = scales::comma, 
                breaks = scales::trans_breaks("log10", function(x) 10^x)) + 
  scale_y_log10(breaks = scales::trans_breaks("log10", function(x) 10^x)) +
  labs(title = "Figure 2: Distribution of Views (Log-Log Scale)",
       subtitle = paste("Tail analysis (x >= ", x_min, ") shows linear descent"),
       x = "Total Views (Log Scale)", 
       y = "Count of Channels (Log Scale)") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"))
```

Figure 1 confirms the extreme positive skewness of the data. The vast majority of channels cluster near zero, while a tiny fraction extends far to the right, making the tail invisible on a standard linear scale.To test the Power-Law hypothesis, we switch to a log-log scale (Figure 2).

A signature feature of Pareto distributions is a straight line on this plot. As observed, the filtered tail ($x \ge 5000$) follows a clear linear descent, providing strong visual evidence for the Power-Law model.

### 3. Inequality Visualization (Lorenz Curve)

To quantify the concentration of views, we construct a Lorenz curve. If views were distributed equally, the curve would follow the diagonal ($y=x$).

```{r}
alpha_8020 <- log(5) / log(4)

lorenz_data <- pareto_data %>%
  arrange(total_views) %>%
  mutate(
    cum_channels = row_number() / n(),
    cum_views = cumsum(total_views) / sum(total_views),
    theoretical_8020 = 1 - (1 - cum_channels)^(1 - 1/alpha_8020)
  )

top_20_share <- lorenz_data %>% 
  filter(cum_channels >= 0.8) %>% 
  head(1) %>% 
  pull(cum_views)

ggplot(lorenz_data, aes(x = cum_channels)) +
  geom_abline(intercept = 0, slope = 1, linetype = "dotted", color = "gray", size = 1) +
  annotate("text", x = 0.3, y = 0.35, label = "Perfect Equality", color = "gray", angle = 45) +

  geom_line(aes(y = theoretical_8020), color = "steelblue", linetype = "dashed", size = 1) +
  annotate("text", x = 0.5, y = 0.18, label = "Classic 80/20 Rule", color = "steelblue") +

  geom_line(aes(y = cum_views), color = "darkred", size = 1.2) +
  
  annotate("text", x = 0.65, y = 0.05, 
           label = paste0("YouTube Reality:\nBottom 80% own ", round(top_20_share*100, 2), "%"), 
           color = "darkred", fontface="bold", hjust = 0) +

  labs(title = "Lorenz Curve: YouTube vs. Classic Pareto",
       subtitle = "Comparing Empirical Data against the standard 80/20 Rule",
       x = "Cumulative % of Channels", y = "Cumulative % of Views") +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()
```

Conclusion: The curve is extremely convex, staying close to the x-axis until the very end. This illustrates the "Winner-Takes-All" nature of YouTube, where the top percentile of creators captures the lion's share of attention.

### 4. Kolmogorov-Smirnov test

Finally, we formally test the goodness of fit using the Kolmogorov-Smirnov (KS) test.

Hypotheses:

$H_0$: The channel views follow a Pareto distribution.

$H_1$: The channel views do not follow a Pareto distribution.

The KS statistic $D$ measures the maximum distance between the empirical CDF and the theoretical CDF:$$D = \max_x |F_{empirical}(x) - F_{theoretical}(x)|$$

```{r}
# Define theoretical Pareto CDF function
ppareto_manual <- function(x, xm, alpha) { ifelse(x < xm, 0, 1 - (xm / x)^alpha) }

# Run KS Test on the filtered tail data
ks_res <- ks.test(jitter(pareto_data$total_views), "ppareto_manual", xm = x_min, alpha = alpha_hat)
print(ks_res)

# Visualization: Empirical vs Theoretical CDF
ggplot(pareto_data, aes(x = total_views)) +
  stat_ecdf(geom = "step", color = "black", size = 1, aes(linetype = "Empirical Data")) +
  stat_function(fun = ppareto_manual, args = list(xm = x_min, alpha = alpha_hat), 
                color = "red", size = 1, aes(linetype = "Theoretical Pareto")) +
  scale_x_log10(labels = scales::comma) +
  labs(title = "KS-Test: Empirical vs Theoretical CDF",
       subtitle = "Visual comparison of the actual data against the ideal Pareto model",
       x = "Total Views (Log Scale)", y = "Cumulative Probability") +
  scale_linetype_manual(name = "Legend", values = c("solid", "dashed"), 
                        guide = guide_legend(override.aes = list(color = c("black", "red")))) +
  theme_minimal() + theme(legend.position = "bottom")
```

### 5. Conclusion

The KS-test yields a p-value $< 2.2 \times 10^{-16}$, leading to a formal rejection of $H_0$. This is a standard outcome when applying strict statistical tests to large datasets ($N > 30,000$), where even minor deviations are flagged as significant.

However, the Log-Log visualization (Figure 2) and the ECDF plot above demonstrate that the theoretical Pareto curve (red dashed line) closely tracks the empirical data. Therefore, we conclude that the Pareto distribution is a strong descriptive model for the trending video ecosystem, confirming the extreme inequality in viewer attention.

## Part 2. Total Effect: The influence of comments on trending persistence

Before analyzing the mediation role of likes (the mechanism), we must first establish the **Total Effect**: Does a higher comment rate actually increase the probability of a video remaining in the trends?

To do this, we model the binary outcome $Y$ (Trending Next Day) directly against the predictor $X$ (Comment Rate).

### 1. Theory: Logistic Regression Model

Since our dependent variable $Y$ is binary (0 or 1), a standard linear regression is unsuitable because it can predict probabilities outside the $[0,1]$ range. Instead, we use **Logistic Regression**.

We model the **log-odds** (logit) of the probability $p = P(Y=1)$ as a linear function of the comment rate $X$:

$$\ln \left( \frac{p}{1-p} \right) = c_0 + c X$$

Where:

-   $p$ is the probability that the video appears in trends the next day.
-   $X$ is the Comment Rate ($\text{comments} / \text{views}$).
-   $c$ represents the change in the log-odds of trending for a unit increase in comment rate.
-   $c_0$ is the intercept.

To get the actual probability, we use the sigmoid function:

$$P(Y=1|X) = \frac{1}{1 + e^{-(c_0 + c X)}}$$

#### Hypotheses

We test whether the "buzz" generated by comments significantly impacts the video's momentum.

-   **Null Hypothesis (**$H_0$): $c = 0$. The comment rate has no effect on the probability of trending the next day.
-   **Alternative Hypothesis (**$H_1$): $c \neq 0$. There is a significant association between comment rate and trending persistence.

### 2. Data Processing

We process the **entire dataset** to generate the necessary variables for this analysis and the subsequent mediation analysis.

-   $X$ (Comment Rate): Normalized by views (times 1000 to make coefficients readable).
-   $M$ (Likes Ratio): Normalized by views.
-   $Y$ (Trending Next Day): Calculated by checking if the same `video_id` appears in the dataset on the subsequent day ($t+1$).

```{r}
View(df)

# сreating X, M, Y
df_processed <- df %>%
  mutate(
    trending_date = as.Date(trending_date, format = "%y.%d.%m"), 
    # scaled by 1000 for easier interpretation of coefficients
    X = (comment_count / views) * 1000,
    M = (likes / views) * 1000
  ) %>%
  # Sort to ensure lead() works correctly
  arrange(country, video_id, trending_date) %>%
  group_by(country, video_id) %>%
  mutate(
    # Check if the next entry for this video is exactly 1 day later
    Y = ifelse(lead(trending_date) == trending_date + 1, 1, 0)
  ) %>%
  ungroup() %>%
  # remove rows with infinite rates or NA
  filter(is.finite(X), is.finite(Y))

# Summary
table_y <- table(df_processed$Y)
cat("Total Observations:", nrow(df_processed), "\n")
cat("Videos NOT trending next day (0):", table_y[1], "\n")
cat("Videos trending next day (1):", table_y[2], "\n")

```

### 3. Model Fitting and Results

We fit the logistic regression model using the **Maximum Likelihood Estimation (MLE)** method provided by R's glm function.

We are testing the Total Effect equation: $$\ln(\text{odds}) = c_0 + c \cdot X$$

# WILL CHANGE

```{r}
# Fit Logistic Regression: Y ~ X
# Family = binomial(link = "logit") handles the binary nature of Y
logit_model_total <- glm(Y ~ X, 
                         data = df_processed, 
                         family = binomial(link = "logit"))

# Display summary
summary(logit_model_total)

# Extract Coefficients for interpretation
c_0 <- coef(logit_model_total)[1] # Intercept
c_total <- coef(logit_model_total)[2] # Slope (Total Effect)
odds_ratio <- exp(c_total)

cat("\n=== Interpretation of Total Effect ===\n")
cat("Intercept (c0):", round(c_0, 4), "\n")
cat("Slope (c - Total Effect):", round(c_total, 4), "\n")
cat("Odds Ratio (exp(c)):", round(odds_ratio, 4), "\n")
cat("Interpretation: An increase of 1 unit in Comment Rate multiplies the odds of trending by", round(odds_ratio, 4), "\n")
```

# WILL CHANGE
```{r}
# Для гарного форматування відсотків на осі Y

# --- Підготовка даних для графіка ---

# 1. Визначаємо розумну межу для осі X (щоб викиди не псували графік)
# Беремо 99.5-й перцентиль даних
x_limit_plot <- quantile(df_processed$X, 0.995, na.rm = TRUE)

# 2. Дані для ТЕОРЕТИЧНОЇ кривої (Синя лінія моделі)
# Створюємо послідовність точок X від мінімуму до нашої межі
x_seq <- seq(min(df_processed$X, na.rm = TRUE), x_limit_plot, length.out = 300)

# Робимо передбачення ймовірності (type = "response") за допомогою моделі
y_pred_curve <- predict(logit_model_total,
                        newdata = data.frame(X = x_seq),
                        type = "response")

curve_data <- data.frame(X = x_seq, prob = y_pred_curve)

# 3. Дані для ЕМПІРИЧНИХ точок (Червоні точки реальності)
# Розбиваємо дані на 25 бінів (груп) і рахуємо середнє в кожній
plot_data_binned <- df_processed %>%
  filter(X <= x_limit_plot) %>% # Фільтруємо екстремальні викиди для візуалізації
  mutate(bin = ntile(X, 25)) %>% # Ділимо на 25 рівних груп
  group_by(bin) %>%
  summarise(
    avg_X = mean(X),            # Центр біна по Х
    observed_prob = mean(Y),    # Реальна частка Y=1 в цьому біні
    n_obs = n(),                # Кількість спостережень (для розміру точки)
    .groups = "drop"
  )

# --- Побудова Графіка ---

ggplot() +
  # Шар 1: Реальні дані (усереднені по бінах)
  geom_point(data = plot_data_binned,
             aes(x = avg_X, y = observed_prob, size = n_obs),
             color = "#E74C3C", # Гарний червоний колір
             alpha = 0.7) +     # Прозорість

  # Шар 2: Теоретична крива моделі
  geom_line(data = curve_data, aes(x = X, y = prob),
            color = "#2980B9", # Гарний синій колір
            linewidth = 1.5) + # Товщина лінії

  # Оформлення
  labs(
    title = "Logistic Regression Fit: Probability of Trending vs. Comment Rate",
    subtitle = "Blue Line: Model Prediction (Sigmoid) | Red Dots: Binned Empirical Averages",
    x = "Comment Rate (X) [scaled]",
    y = "Probability of Trending Next Day P(Y=1)",
    size = "Observations\nin Bin"
  ) +
  # Форматуємо вісь Y у відсотки і фіксуємо межі від 0 до 1
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits = c(0, NA)) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    legend.position = "bottom",
    panel.grid.minor = element_blank() # Прибираємо дрібну сітку
  )
```


# WILL CHANGE
```{r}
# === Manual Calculation of Wald Statistic & P-value ===

# 1. Get Standard Error (SE) from the Variance-Covariance Matrix
# The vcov() function gives the covariance matrix of the model parameters
se_c <- sqrt(vcov(logit_model_total)["X", "X"])

# 2. Calculate Z-statistic (Wald statistic)
# Formula: z = estimate / standard_error
z_score <- c_total / se_c

# 3. Calculate P-value (Two-tailed test)
# We use the Standard Normal Distribution N(0,1)
# Formula: p = 2 * (1 - CDF(|z|))
p_val <- 2 * (1 - pnorm(abs(z_score)))

cat("\n=== Manual Statistical Test Verification ===\n")
cat("Coefficient (c):", c_total, "\n")
cat("Standard Error (SE):", se_c, "\n")
cat("Calculated Z-score:", z_score, "\n")
cat("Calculated P-value:", p_val, "\n")
```

### 4. Conclusions on Total Effect

Based on the logistic regression model results, we can draw the following conclusions regarding the **Total Effect** of comments on trending persistence:

1.  **Statistical Significance ($p$-value & $z$-score):**
    The calculated $z$-score is **21.59**, which corresponds to a $p$-value of effectively **zero** ($< 2 \times 10^{-16}$). This provides overwhelming evidence to **reject the Null Hypothesis ($H_0$)**. We conclude that there is a highly statistically significant relationship between the comment rate and the probability of a video remaining in the trends.

2.  **Direction of Effect ($c$):**
    The coefficient for comment rate is positive ($c \approx 0.0425$). This confirms that higher viewer engagement (in the form of comments) is associated with a higher probability of the video appearing in the trends on the following day.

Now, in **Part 3**, we will investigate the **mechanism**: Does this happen simply because people comment, or does the discussion drive more *likes*, which in turn signals the algorithm to keep the video trending?




## Part 3. The mediation of likes between comment and "go to next day in trends" probability.

We test whether likes serve as an effective mediator explaining how the number of comments influences a video’s probability of appearing in the trends on the following day.

### 1. Theory and variables for research

In this section, we examine whether the effect of the comment rate\
$$X = comments / views$$\
on the probability that a video reappears in trending the following day\
$$Y = trending\_next\_day$$\
is transmitted indirectly through the likes ratio\
$$M = likes / views.$$

The probability of appearing in trending the next day is defined as a binary event:

$$
Y_i =
\begin{cases}
1, & \text{if video } i \text{ appears again on day } t+1 \\
0, & \text{otherwise}
\end{cases}
$$

and conceptually represents\
$$
\underbrace{P(Y=1)}_{\text{probability to reappear in trending tomorrow}}.
$$

This metric captures *momentum*: a video that stays in trending is favored by YouTube’s algorithm.

#### Hypotheses

We test whether likes act as a statistical mediator:

-   **Null hypothesis**

$$H_0: a \cdot b = 0$$

-   **Alternative hypothesis**

$$H_1: a \cdot b \ne 0$$

Here, $a$ and $b$ are coefficients in the mediation pathway.

#### Model Specification

The mediation structure follows the classical framework:

1.  **Effect of comments on likes** (mediator model):

$$
M = aX + \varepsilon_M
$$

2.  **Effect of comments and likes on trending probability** (outcome model):

$$
logit(P(Y=1)) = c'X + bM + \varepsilon_Y
$$

where

$$
logit(p) = \ln \frac{p}{1 - p}
$$

transforms probabilities (bounded between 0 and 1) into real numbers so that they can be modeled as a linear combination of predictors.

3.  **Total effect of comments** (model without mediator):

$$
logit(P(Y=1)) = cX + \varepsilon.
$$

The coefficients have the following interpretation:

-   $a$: how much comments increase likes\
-   $b$: how much likes increase trending probability\
-   $c$: total effect of comments\
-   $c'$: direct effect of comments after controlling for likes

#### Evaluating the Total Effect of Comments

The causal effects are defined as:

-   **Indirect effect (IE)**:\
    $$
    IE = a \cdot b
    $$

-   **Direct effect (DE)**:\
    $$
    DE = c'
    $$

-   **Total effect (TE)**:\
    $$
    TE = c
    $$

These quantities satisfy the identity:

$$
TE = DE + IE.
$$

This equality **always holds** by construction of the regression models; it is not itself a hypothesis test.

What we test instead is:

> **How much of the total effect** $c$ is transmitted through the mediator?

If $IE$ is large and significant → likes explain part of the effect of comments.

If $IE = 0$ → likes do not mediate anything.

If both $DE$ and $IE$ are significant → partial mediation.

If $DE$ becomes small or non-significant while $IE$ is significant → full mediation.

#### Why We Test Only the Indirect Effect $a \cdot b$

The identity\
$$TE = DE + IE$$\
is always mathematically true, so we do **not** test this equality. Instead, we test whether:

$$
IE = a \cdot b \ne 0.
$$

If $IE$ is non-zero → *likes mediate the effect*.

If $IE = 0$ → *likes do not mediate anything*.

#### Testing the Indirect Effect: Bootstrap Procedure

Because the product $a \cdot b$ does not follow a normal distribution, we assess its significance using bootstrap resampling.

We repeatedly (e.g., 10,000 times):

1.  Sample videos **with replacement**
2.  Refit both regression models to obtain new estimates $a^*$ and $b^*$
3.  Compute the bootstrap indirect effect:

$$
IE^* = a^* \cdot b^*.
$$

The bootstrap-based $p$-value is

$$
p = P(|a \cdot b| \le |a^* \cdot b^*|),
$$

where the probability is taken over the bootstrap distribution.

If the bootstrap confidence interval excludes zero → **significant mediation**.

#### Summary Diagram of Effects

$$
X \longrightarrow M \longrightarrow Y
$$

with:

$$
\begin{aligned}
a &: X \rightarrow M \\
b &: M \rightarrow Y \\
c &: X \rightarrow Y \ (\text{total}) \\
c' &: X \rightarrow Y \ (\text{direct})
\end{aligned}
$$

#### Conclusion

If the indirect effect $a \cdot b$ is statistically significant, then:

> **The likes ratio partially mediates the influence of comment rate on a video's probability of reappearing in trending on the next day.**

Otherwise:

> **Comment activity and likes contribute independently to trending recurrence.**

### 2. Data processing and vizualization

add comment_rate, like ratio, trending_next_day

```{r}
# Вибираємо 5000 унікальних video_id випадковим чином
sample_videos <- sample(unique(df$video_id), 100000)

# Фільтруємо датафрейм для цих відео
df_mediation <- df %>% filter(video_id %in% sample_videos)

# Створення похідних змінних
df_mediation <- df_mediation %>%
  mutate(
    comment_rate = (comment_count / views)*1000,
    likes_ratio = (likes / views)*1000,
    trending_date = ymd_hms(publish_time, tz = "UTC")
  )

# Позначаємо, чи відео потрапило в тренди на наступний день
df_mediation <- df_mediation %>%
  arrange(video_id, trending_date) %>%
  group_by(video_id) %>%
  mutate(
    trending_next_day = lead(trending_date, 1) <= (trending_date + days(1)) &
                        lead(trending_date, 1) > trending_date
  ) %>%
  ungroup()

df_mediation$trending_next_day <- as.numeric(df_mediation$trending_next_day)

head(df_mediation)


```

Vizual - Distribution of Comment Counts

```{r}

ggplot(df_mediation, aes(x = comment_count)) +
  geom_histogram(bins = 50, fill = "skyblue", color = "black") +
  scale_x_log10() +
  labs(title = "Distribution of Comment Counts",
       x = "Comment Count (log scale)", y = "Frequency")

```

vizual Top Channels by Videos Trending Next Day + distribution of likes

```{r}
# Histogram of likes
ggplot(df_mediation, aes(x = likes)) +
  geom_histogram(bins = 50, fill = "salmon", color = "black") +
  scale_x_log10() +
  labs(title = "Distribution of Likes",
       x = "Likes (log scale)", y = "Frequency")

# Top 10 channels by number of next-day trending videos
top_channels <- df_mediation %>%
  filter(trending_next_day == 1) %>%
  group_by(channel_title) %>%
  summarise(n_videos = n()) %>%
  arrange(desc(n_videos)) %>%
  slice(1:10)

ggplot(top_channels, aes(x = reorder(channel_title, n_videos), y = n_videos)) +
  geom_bar(stat="identity", fill="purple") +
  coord_flip() +
  labs(title="Top Channels by Videos Trending Next Day",
       x="Channel", y="Number of Videos")


```

vizual comments rate vs trend in next day

```{r}

# Розмір датасету
cat("Number of rows in dataset:", nrow(df_mediation), "\n")

# Підрахунок відео за статусом наступного дня
table_trending <- table(df_mediation$trending_next_day)
table_trending

# Альтернативно, вивести у відсотках
prop.table(table_trending)


```

comments vs likes

```{r}

ggplot(df_mediation, aes(x=comment_rate, y=likes_ratio)) +
  geom_point(alpha=0.3) +
  labs(title="Likes Ratio vs Comment Rate", x="Comment Rate", y="Likes Ratio")


```

### 3. Finding needed parameters

So we created dataset where we are having for every video his trending_next_day, like_ratio, and comment_ratio.

#### linear Regression model for comments and likes

Here we want to find Mediator(likes) dependency of Comments $$
M = aX + a_0
$$

```{r}
# Let`s find a
mean_of_comments = mean(df_mediation$comment_rate)
mean_of_likes = mean(df_mediation$likes_ratio)


a = sum((df_mediation$comment_rate - mean_of_comments)*(df_mediation$likes_ratio - mean_of_likes))/sum((df_mediation$comment_rate - mean_of_comments)^2)

a0 = mean_of_likes * 1000 - a*mean_of_comments* 1000

cat("a = ", a, "\n")
cat("a0 = ", a0 , "\n")
lm_fit <- lm(likes_ratio ~ comment_rate, data=df_mediation)

ggplot(df_mediation, aes(x = comment_rate*1000, y = likes_ratio*1000)) +
  geom_point(alpha = 0.3) +
  geom_abline(intercept = a0, slope = a, color = "red", size = 1) +
  labs(
    title = "Likes Ratio vs Comment Rate",
    x = "Comment Rate * 1000",
    y = "Likes Ratio * 1000"
  ) +
  theme_minimal()


summary(lm_fit)

```

#####logit regression for comments - likes -\> trending_next_day

```{r}
# ========================================
# MLE ОЦІНКИ ДЛЯ ЛОГІСТИЧНОЇ МЕДІАЦІЇ
# ========================================

# Підготовка даних
X_original <- df_mediation$comment_rate
M_original <- df_mediation$likes_ratio
Y <- df_mediation$trending_next_day

# Видалення NA
valid_idx <- !is.na(X_original) & !is.na(M_original) & !is.na(Y)
X_original <- X_original[valid_idx]
M_original <- M_original[valid_idx]
Y <- Y[valid_idx]

cat("Кількість спостережень:", length(Y), "\n")
cat("Y=1:", sum(Y), ", Y=0:", sum(1-Y), "\n\n")

# Масштабування × 1000
X <- X_original * 1000
M <- M_original * 1000
n <- length(X)

# ========================================
# КРОК 1: M ~ X (лінійна регресія)
# ========================================

x_bar <- mean(X)
m_bar <- mean(M)
y_bar <- mean(Y)

X_tilde <- X - x_bar
M_tilde <- M - m_bar

S_xx <- sum(X_tilde^2)
S_xm <- sum(X_tilde * M_tilde)

a <- S_xm / S_xx
a0 <- m_bar - a * x_bar

cat("=== M ~ X ===\n")
cat("a =", a, "\n")
cat("a0 =", a0, "\n\n")

# ========================================
# КРОК 2: Y ~ X + M (логістична регресія IRLS)
# ========================================

# Початкові оцінки (МНК)
Y_tilde <- Y - y_bar
S_mm <- sum(M_tilde^2)
S_xy <- sum(X_tilde * Y_tilde)
S_my <- sum(M_tilde * Y_tilde)

D <- S_xx * S_mm - S_xm^2

b <- (S_xx * S_my - S_xm * S_xy) / D
c_prime <- (S_mm * S_xy - S_xm * S_my) / D
intercept_Y <- y_bar - c_prime * x_bar - b * m_bar

# IRLS ітерації
epsilon <- 1e-6
max_iter <- 50
iter <- 0

repeat {
  iter <- iter + 1
  
  c_prime_old <- c_prime
  b_old <- b
  intercept_Y_old <- intercept_Y
  
  # Предиктор та ймовірності
  eta <- c_prime * X + b * M + intercept_Y
  eta <- pmax(pmin(eta, 20), -20)
  p <- 1 / (1 + exp(-eta))
  p <- pmax(pmin(p, 1 - 1e-10), 1e-10)
  
  # Ваги та working response
  w <- p * (1 - p)
  w <- pmax(w, 1e-8)
  u <- eta + (Y - p) / w
  
  if (any(is.na(u))) {
    c_prime <- c_prime_old
    b <- b_old
    intercept_Y <- intercept_Y_old
    break
  }
  
  # Зважені середні
  w_sum <- sum(w)
  x_bar_w <- sum(w * X) / w_sum
  m_bar_w <- sum(w * M) / w_sum
  u_bar_w <- sum(w * u) / w_sum
  
  X_tilde_w <- X - x_bar_w
  M_tilde_w <- M - m_bar_w
  u_tilde_w <- u - u_bar_w
  
  # Зважені суми
  S_xx_w <- sum(w * X_tilde_w^2)
  S_mm_w <- sum(w * M_tilde_w^2)
  S_xm_w <- sum(w * X_tilde_w * M_tilde_w)
  S_xu_w <- sum(w * X_tilde_w * u_tilde_w)
  S_mu_w <- sum(w * M_tilde_w * u_tilde_w)
  
  if (any(is.na(c(S_xx_w, S_mm_w, S_xm_w, S_xu_w, S_mu_w)))) {
    c_prime <- c_prime_old
    b <- b_old
    intercept_Y <- intercept_Y_old
    break
  }
  
  D_w <- S_xx_w * S_mm_w - S_xm_w^2
  
  if (is.na(D_w) || abs(D_w) < 1e-10) {
    c_prime <- c_prime_old
    b <- b_old
    intercept_Y <- intercept_Y_old
    break
  }
  
  # Оновлені оцінки
  b <- (S_xx_w * S_mu_w - S_xm_w * S_xu_w) / D_w
  c_prime <- (S_mm_w * S_xu_w - S_xm_w * S_mu_w) / D_w
  intercept_Y <- u_bar_w - c_prime * x_bar_w - b * m_bar_w
  
  # Перевірка збіжності
  delta_b <- abs(b - b_old)
  delta_c <- abs(c_prime - c_prime_old)
  delta_int <- abs(intercept_Y - intercept_Y_old)
  
  if (is.na(delta_b) || is.na(delta_c) || is.na(delta_int)) {
    c_prime <- c_prime_old
    b <- b_old
    intercept_Y <- intercept_Y_old
    break
  }
  
  if (iter >= max_iter) break
  if (delta_b < epsilon && delta_c < epsilon && delta_int < epsilon) break
}

cat("=== Y ~ X + M (збіжність на ітерації", iter, ") ===\n")
cat("c_prime (масштабований) =", c_prime, "\n")
cat("b (масштабований) =", b, "\n")
cat("intercept_Y =", intercept_Y, "\n\n")

# У вихідному масштабі
c_prime_original <- c_prime * 1000
b_original <- b * 1000

cat("=== Оцінки у вихідному масштабі ===\n")
cat("c_prime =", c_prime_original, "\n")
cat("b =", b_original, "\n")
cat("intercept_Y =", intercept_Y, "\n\n")

# Медіаційні ефекти
IE <- (a * 1000) * (b * 1000) / 1000
DE <- c_prime * 1000


cat("=== Медіаційні ефекти ===\n")
cat("Indirect Effect (a × b) =", IE, "\n")
cat("Direct Effect (c') =", DE, "\n")

# Перевірка з glm
logit_outcome <- glm(Y ~ X_original + M_original, family = binomial(link = "logit"))
cat("=== Порівняння з glm() ===\n")
print(coef(logit_outcome))
cat("\n")

# Статистика
cat("=== Статистика прогнозів ===\n")
cat("Мін P(Y=1):", min(p_pred), "\n")
cat("Медіана P(Y=1):", median(p_pred), "\n")
cat("Середня P(Y=1):", mean(p_pred), "\n")
cat("Макс P(Y=1):", max(p_pred), "\n")
cat("Частка Y=1 у даних:", mean(Y), "\n")
```

graphs

```{r}
# ========================================
# ГРАФІКИ
# ========================================

par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# 1. M ~ X
plot(X_original, M_original, pch = 16, cex = 0.3, col = rgb(0, 0, 0, 0.2),
     xlab = "Comment Rate", ylab = "Likes Ratio", main = "Медіатор M ~ X")
abline(a = a0/1000, b = a, col = "blue", lwd = 2)
legend("topleft", legend = paste0("M = ", round(a0/1000, 3), " + ", round(a, 3), " × X"),
       col = "blue", lwd = 2, bty = "n")

# 2. Predicted probabilities (з трендом)
eta_pred <- c_prime_original * X_original + b_original * M_original + intercept_Y
p_pred <- 1 / (1 + exp(-eta_pred))

# Бінування для тренду
x_bins <- cut(X_original, breaks = 20, include.lowest = TRUE)
x_bin_mid <- tapply(X_original, x_bins, mean)
p_bin_mean <- tapply(p_pred, x_bins, mean)
y_bin_obs <- tapply(Y, x_bins, mean)

plot(X_original, p_pred, pch = 16, cex = 0.3, col = rgb(0, 0, 1, 0.15),
     xlab = "Comment Rate", ylab = "P(Y=1)", main = "Прогнозована ймовірність",
     ylim = c(0, max(p_bin_mean, y_bin_obs, na.rm = TRUE) * 1.2))
lines(x_bin_mid, p_bin_mean, col = "blue", lwd = 3)
points(x_bin_mid, y_bin_obs, col = "red", pch = 19, cex = 1.2)
legend("topleft", legend = c("Прогноз (тренд)", "Спостережувана частота"), 
       col = c("blue", "red"), lwd = c(3, NA), pch = c(NA, 19), bty = "n")

# 3. Розподіл predicted probabilities (логарифмічна шкала)
hist(log10(p_pred + 1e-10), breaks = 50, col = "lightblue", border = "white",
     xlab = "log10(P(Y=1))", main = "Розподіл ймовірностей (log-шкала)")
abline(v = log10(mean(p_pred)), col = "red", lwd = 2, lty = 2)

# 4. Калібраційний графік
p_bins <- cut(p_pred, breaks = quantile(p_pred, probs = seq(0, 1, 0.1)), include.lowest = TRUE)
obs_freq <- tapply(Y, p_bins, mean)
pred_freq <- tapply(p_pred, p_bins, mean)

plot(pred_freq, obs_freq, pch = 19, cex = 1.5, col = "darkgreen",
     xlab = "Прогнозована P(Y=1)", ylab = "Спостережувана частота Y=1", 
     main = "Калібрація моделі", xlim = c(0, max(pred_freq)*1.1), ylim = c(0, max(obs_freq, na.rm=TRUE)*1.1))
abline(0, 1, lty = 2, col = "gray", lwd = 2)
grid()

par(mfrow = c(1, 1))

```
