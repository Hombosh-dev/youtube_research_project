---
title: "youtube_research_project"
author: "Kosiv Marta, Hombosh Oleh, Kolodchak Bohdan"
date: "`r Sys.Date()`"
output: html_document
---

## Project Description

##### Data Source: Kaggle: [Kaggle: Trending YouTube Video Statistics](https://www.kaggle.com/datasets/datasnaek/youtube-new)

For our research project, we have selected the Trending YouTube Video Statistics dataset. This dataset contains information about the daily trending YouTube videos in several countries (US, GB, DE, CA). It provides variables such as: views, likes, dislikes, comment_count, tags, category_id.

This data allows us to analyze what makes a video popular and test different hypotheses about viewer engagement.

Some useful packages:

```{r}
library(tidyverse)
library(readr)
library(lubridate)
library(ggplot2)
library(dplyr)
library(scales)
```

The first step was to read data and look what actually we can analyze.

```{r}
path <- "dataset/"
files <- list.files(path, pattern = "*.csv", full.names = TRUE)

df_list <- lapply(files, function(file) {
  data <- read_csv(file, show_col_types = FALSE)
  
  country_name <- file %>%
    basename() %>%
    substr(1, 2)
  
  data$country <- country_name
  return(data)
})

df <- bind_rows(df_list)

print(head(df))
```

## Part I. Pareto Principle (80/20)

We analyze the inequality of channel views to test if they follow a **Power-Law (Pareto) distribution**.

### 1. Theory & Parameter Estimation (MLE)

#### Mathematical Derivation of MLE Estimator

To justify our calculation of $\alpha$, we derive the Maximum Likelihood Estimator (MLE) from the Pareto Probability Density Function (PDF).

**1. The Likelihood Function** Given a sample of observations $x_1, x_2, ..., x_n$ following a Pareto distribution with PDF $f(x) = \frac{\alpha x_m^\alpha}{x^{\alpha+1}}$, the likelihood function $L(\alpha)$ is the product of individual probabilities:

$$L(\alpha) = \prod_{i=1}^n \frac{\alpha x_m^\alpha}{x_i^{\alpha+1}} = \alpha^n x_m^{n\alpha} \prod_{i=1}^n x_i^{-(\alpha+1)}$$

**2. The Log-Likelihood Function** It is computationally easier to maximize the log-likelihood ($\ln L$). Taking the natural logarithm transforms the product into a sum:

$$\ln L(\alpha) = n \ln \alpha + n\alpha \ln x_m - (\alpha+1) \sum_{i=1}^n \ln x_i$$

**3. Maximization** To find the optimal $\alpha$, we take the derivative with respect to $\alpha$ and set it to zero:

$$\frac{\partial \ln L}{\partial \alpha} = \frac{n}{\alpha} + n \ln x_m - \sum_{i=1}^n \ln x_i = 0$$

**4. Solving for** $\alpha$ Rearranging the terms allows us to isolate $\hat{\alpha}$:

$$\frac{n}{\hat{\alpha}} = \sum_{i=1}^n \ln x_i - n \ln x_m = \sum_{i=1}^n (\ln x_i - \ln x_m)$$ $$\frac{n}{\hat{\alpha}} = \sum_{i=1}^n \ln \left( \frac{x_i}{x_m} \right)$$

Finally, we obtain the formula used in our code:

$$\hat{\alpha} = \frac{n}{\sum_{i=1}^n \ln(x_i / x_m)}$$

where $x_m$ is the minimum value and $\alpha$ is the shape parameter (tail index).

Below, we perform this calculation manually in R.

### 2. Data Processing & Visualization

#### Data processing

First, we aggregate the data to handle duplicates. Since the dataset contains daily snapshots, the same video appears multiple times. To avoid double-counting, we extract the **peak (maximum) views** for each unique video before summing them up for the channel.

```{r}
channel_stats <- df %>%
  group_by(video_id) %>%
  summarise(
    channel_title = first(channel_title),
    max_video_views = max(views, na.rm = TRUE), # Peak views for the video
    .groups = "drop"
  ) %>%
  group_by(channel_title) %>%
  summarise(total_views = sum(max_video_views, na.rm = TRUE)) %>%
  filter(total_views > 0) %>%
  arrange(desc(total_views))

x_min <- 5000 
pareto_data <- channel_stats %>% filter(total_views >= x_min)

n <- nrow(pareto_data)

log_sum <- sum(log(pareto_data$total_views / x_min))
alpha_hat <- n / log_sum

print(paste("Sample size (tail n):", n))
print(paste("Minimum views (x_min):", x_min))
print(paste("Estimated Alpha (Shape Parameter):", round(alpha_hat, 4)))
```

The calculated shape parameter $\alpha \approx 0.28$ indicates an extremely heavy tail. In standard Pareto distributions, $\alpha$ typically ranges between 1 and 3. A value below 1 implies that the mathematical mean of the distribution is infinite. This confirms that the YouTube ecosystem is dominated by "black swan" events-viral videos that skew all average metrics.

#### Figure 1: Distribution of Views (Linear Scale)

As described in the problem statement, we first observe the distribution on a standard linear scale to identify skewness.

```{r}
ggplot(channel_stats, aes(x = total_views)) +
  geom_histogram(bins = 50, fill = "steelblue", color = "white") +
  scale_x_continuous(labels = scales::comma) +
  labs(title = "Figure 1: Distribution of Total Views (Linear Scale)",
       subtitle = "Extreme positive skewness suggests a heavy-tailed distribution",
       x = "Total Views", y = "Count of Channels") +
  theme_minimal()
```



#### Figure 2: Log-Log Scale Analysis

To confirm the Pareto law, we plot the tail data ($x \ge x_{min}$) on a log-log scale. A straight line indicates adherence to the power law.

```{r}
ggplot(pareto_data, aes(x = total_views)) +
  geom_histogram(bins = 50, fill = "#69b3a2", color = "white", alpha = 0.8) +
  scale_x_log10(labels = scales::comma, 
                breaks = scales::trans_breaks("log10", function(x) 10^x)) + 
  scale_y_log10(breaks = scales::trans_breaks("log10", function(x) 10^x)) +
  labs(title = "Figure 2: Distribution of Views (Log-Log Scale)",
       subtitle = paste("Tail analysis (x >= ", x_min, ") shows linear descent"),
       x = "Total Views (Log Scale)", 
       y = "Count of Channels (Log Scale)") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold"))
```

Figure 1 confirms the extreme positive skewness of the data. The vast majority of channels cluster near zero, while a tiny fraction extends far to the right, making the tail invisible on a standard linear scale.To test the Power-Law hypothesis, we switch to a log-log scale (Figure 2).

A signature feature of Pareto distributions is a straight line on this plot. As observed, the filtered tail ($x \ge 5000$) follows a clear linear descent, providing strong visual evidence for the Power-Law model.

### 3. Inequality Visualization (Lorenz Curve)

To quantify the concentration of views, we construct a Lorenz curve. If views were distributed equally, the curve would follow the diagonal ($y=x$).

```{r}
alpha_8020 <- log(5) / log(4)

lorenz_data <- pareto_data %>%
  arrange(total_views) %>%
  mutate(
    cum_channels = row_number() / n(),
    cum_views = cumsum(total_views) / sum(total_views),
    theoretical_8020 = 1 - (1 - cum_channels)^(1 - 1/alpha_8020)
  )

top_20_share <- lorenz_data %>% 
  filter(cum_channels >= 0.8) %>% 
  head(1) %>% 
  pull(cum_views)

ggplot(lorenz_data, aes(x = cum_channels)) +
  geom_abline(intercept = 0, slope = 1, linetype = "dotted", color = "gray", size = 1) +
  annotate("text", x = 0.3, y = 0.35, label = "Perfect Equality", color = "gray", angle = 45) +

  geom_line(aes(y = theoretical_8020), color = "steelblue", linetype = "dashed", size = 1) +
  annotate("text", x = 0.5, y = 0.18, label = "Classic 80/20 Rule", color = "steelblue") +

  geom_line(aes(y = cum_views), color = "darkred", size = 1.2) +
  
  annotate("text", x = 0.65, y = 0.05, 
           label = paste0("YouTube Reality:\nBottom 80% own ", round(top_20_share*100, 2), "%"), 
           color = "darkred", fontface="bold", hjust = 0) +

  labs(title = "Lorenz Curve: YouTube vs. Classic Pareto",
       subtitle = "Comparing Empirical Data against the standard 80/20 Rule",
       x = "Cumulative % of Channels", y = "Cumulative % of Views") +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  theme_minimal()
```
The Lorenz Curve provides a visual representation of the inequality in viewership distribution. The graph compares our empirical data (dark red line) against two benchmarks: the line of Perfect Equality (gray dotted diagonal, where everyone gets equal views) and the standard Classic 80/20 Pareto Rule (blue dashed curve).

The empirical curve hugs the x-axis much more tightly than the classic Pareto model. This indicates that the inequality on YouTube is significantly more severe than in standard economic scenarios.

As annotated on the graph, the bottom 80% of channels capture only 7.31% of the total views. Conversely, this implies that the top 20% of channels dominate approximately 92.7% of all viewer attention.

Conclusion: The curve is extremely convex, staying close to the x-axis until the very end. This illustrates the "Winner-Takes-All" nature of YouTube, where the top percentile of creators captures the lion's share of attention.


### 4. Kolmogorov-Smirnov test

Finally, we formally test the goodness of fit using the Kolmogorov-Smirnov (KS) test.

Hypotheses:

$H_0$: The channel views follow a Pareto distribution.

$H_1$: The channel views do not follow a Pareto distribution.

The KS statistic $D$ measures the maximum distance between the empirical CDF and the theoretical CDF:$$D = \max_x |F_{empirical}(x) - F_{theoretical}(x)|$$

```{r}
# Define theoretical Pareto CDF function
ppareto_manual <- function(x, xm, alpha) { ifelse(x < xm, 0, 1 - (xm / x)^alpha) }

# Run KS Test on the filtered tail data
ks_res <- ks.test(jitter(pareto_data$total_views), "ppareto_manual", xm = x_min, alpha = alpha_hat)
print(ks_res)

# Visualization: Empirical vs Theoretical CDF
ggplot(pareto_data, aes(x = total_views)) +
  stat_ecdf(geom = "step", color = "black", size = 1, aes(linetype = "Empirical Data")) +
  stat_function(fun = ppareto_manual, args = list(xm = x_min, alpha = alpha_hat), 
                color = "red", size = 1, aes(linetype = "Theoretical Pareto")) +
  scale_x_log10(labels = scales::comma) +
  labs(title = "KS-Test: Empirical vs Theoretical CDF",
       subtitle = "Visual comparison of the actual data against the ideal Pareto model",
       x = "Total Views (Log Scale)", y = "Cumulative Probability") +
  scale_linetype_manual(name = "Legend", values = c("solid", "dashed"), 
                        guide = guide_legend(override.aes = list(color = c("black", "red")))) +
  theme_minimal() + theme(legend.position = "bottom")
```

### 5. Conclusion

The KS-test yields a p-value $< 2.2 \times 10^{-16}$, leading to a formal rejection of $H_0$. This is a standard outcome when applying strict statistical tests to large datasets ($N > 30,000$), where even minor deviations are flagged as significant.

However, the Log-Log visualization (Figure 2) and the ECDF plot above demonstrate that the theoretical Pareto curve (red dashed line) closely tracks the empirical data. Therefore, we conclude that the Pareto distribution is a strong descriptive model for the trending video ecosystem, confirming the extreme inequality in viewer attention.

## Part 2. Total Effect: The influence of comments on trending persistence

Before analyzing the mediation role of likes (the mechanism), we must first establish the **Total Effect**: Does a higher comment rate actually increase the probability of a video remaining in the trends?

To do this, we model the binary outcome $Y$ (Trending Next Day) directly against the predictor $X$ (Comment Rate).

### 1. Theory: Logistic Regression Model

Since our dependent variable $Y$ is binary (0 or 1), a standard linear regression is unsuitable because it can predict probabilities outside the $[0,1]$ range. Instead, we use **Logistic Regression**.

We model the **log-odds** (logit) of the probability $p = P(Y=1)$ as a linear function of the comment rate $X$:

$$\ln \left( \frac{p}{1-p} \right) = c_0 + c X$$

Where:

-   $p$ is the probability that the video appears in trends the next day.
-   $X$ is the Comment Rate ($\text{comments} / \text{views}$).
-   $c$ represents the change in the log-odds of trending for a unit increase in comment rate.
-   $c_0$ is the intercept.

To get the actual probability, we use the sigmoid function:

$$P(Y=1|X) = \frac{1}{1 + e^{-(c_0 + c X)}}$$

### 2. Data Processing

We process the **entire dataset** to generate the necessary variables for this analysis and the subsequent mediation analysis.

-   $X$ (Comment Rate): Normalized by views (times 1000 to make coefficients readable).
-   $M$ (Likes Ratio): Normalized by views.
-   $Y$ (Trending Next Day): Calculated by checking if the same `video_id` appears in the dataset on the subsequent day ($t+1$).

```{r}
# сreating X, M, Y
df_processed <- df %>%
  mutate(
    trending_date = as.Date(trending_date, format = "%y.%d.%m"), 
    # scaled by 1000 for easier interpretation of coefficients
    X = (comment_count / views) * 1000,
    M = (likes / views) * 1000
  ) %>%
  # Sort to ensure lead() works correctly
  arrange(country, video_id, trending_date) %>%
  group_by(country, video_id) %>%
  mutate(
    # Check if the next entry for this video is exactly 1 day later
    Y = ifelse(lead(trending_date) == trending_date + 1, 1, 0)
  ) %>%
  ungroup() %>%
  # remove rows with infinite rates or NA
  filter(is.finite(X), is.finite(Y))

# Summary
table_y <- table(df_processed$Y)
cat("Total Observations:", nrow(df_processed), "\n")
cat("Videos NOT trending next day (0):", table_y[1], "\n")
cat("Videos trending next day (1):", table_y[2], "\n")

```

### 3. Statistical Inference: The Wald Test

To assess the statistical significance of the Total Effect coefficient ($c$), R's `glm` function utilizes the **Wald Test**. This test evaluates whether the estimated coefficient is significantly different from zero given its standard error.

#### a. Hypotheses

We test the following:\
**Null Hypothesis (**$H_0$): $c = 0$. The comment rate has no influence on the probability of trending the next day.\
**Alternative Hypothesis (**$H_1$): $c \neq 0$. The comment rate significantly influences the outcome.

#### b. The Wald Statistic ($z$-score)

The test statistic is calculated by dividing the estimated coefficient ($\hat{c}$) by its standard error ($SE$). Under the null hypothesis and assuming a large sample size (asymptotic normality via the Central Limit Theorem), this ratio follows a **Standard Normal Distribution** ($N(0,1)$).

$$
z = \frac{\hat{c}}{SE(\hat{c})}
$$

Where:\
$\hat{c}$ is the Maximum Likelihood Estimate of the coefficient.\
$SE(\hat{c})$ is the standard error, derived from the inverse of the Fisher Information Matrix (variance-covariance matrix).

#### c. P-value Calculation

The $p$-value represents the probability of observing a test statistic as extreme as, or more extreme than, the one calculated, assuming $H_0$ is true. Since we are testing for *any* effect (positive or negative), we perform a **two-tailed test**:

$$
p\text{-value} = 2 \cdot (1 - \Phi(|z|))
$$

Where $\Phi$ is the Cumulative Distribution Function (CDF) of the standard normal distribution.

#### d. Decision Rule

-   If $p\text{-value} < \alpha$ (typically $0.05$), we **reject** $H_0$ and conclude that the relationship is statistically significant.

```{r}
# Fit Logistic Regression: Y ~ X
logit_model_total <- glm(Y ~ X, 
                         data = df_processed, 
                         family = binomial(link = "logit"))

# Display summary
summary(logit_model_total)

# Extract Coefficients for interpretation
c_0 <- coef(logit_model_total)[1]
c_total <- coef(logit_model_total)[2]
odds_ratio <- exp(c_total)

cat("\n=== Interpretation of Total Effect ===\n")
cat("Intercept (c0):", round(c_0, 4), "\n")
cat("Slope (c - Total Effect):", round(c_total, 4), "\n")
cat("Odds Ratio (exp(c)):", round(odds_ratio, 4), "\n")
cat("Interpretation: An increase of 1 unit in Comment Rate multiplies the odds of trending by", round(odds_ratio, 4), "\n")
```

Visualization:

```{r}
# Data Preparation for Plotting

# define a reasonable limit for the X-axis
x_limit_plot <- quantile(df_processed$X, 0.995, na.rm = TRUE)

# data for the theoretical curve (Blue model line)
# Create a sequence of X points from the minimum to our limit
x_seq <- seq(min(df_processed$X, na.rm = TRUE), x_limit_plot, length.out = 300)

# predict probabilities using the model
y_pred_curve <- predict(logit_model_total,
                        newdata = data.frame(X = x_seq),
                        type = "response")

curve_data <- data.frame(X = x_seq, prob = y_pred_curve)

# data for empirical points (red dots representing reality)
# Split data into 25 bins (groups) and calculate the mean in each
plot_data_binned <- df_processed %>%
  filter(X <= x_limit_plot) %>% 
  mutate(bin = ntile(X, 25)) %>% # divide into 25 equal-sized groups
  group_by(bin) %>%
  summarise(
    avg_X = mean(X),            # bin center for X
    observed_prob = mean(Y),    # actual proportion of Y=1 in this bin
    n_obs = n(),                # number of observations (for dot size)
    .groups = "drop"
  )

# Plot Construction

ggplot() +
  # empirical data (averaged by bin)
  geom_point(data = plot_data_binned,
             aes(x = avg_X, y = observed_prob, size = n_obs),
             color = "#E74C3C",
             alpha = 0.7) +

  # theoretical model curve
  geom_line(data = curve_data, aes(x = X, y = prob),
            color = "#2980B9",
            linewidth = 1.5) +

  
  labs(
    title = "Logistic Regression Fit: Probability of Trending vs. Comment Rate",
    subtitle = "Blue Line: Model Prediction (Sigmoid) | Red Dots: Binned Empirical Averages",
    x = "Comment Rate (X) [scaled]",
    y = "Probability of Trending Next Day P(Y=1)",
    size = "Observations\nin Bin"
  ) +
  # format Y-axis as percentages and fix limits from 0 to 1
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits = c(0, NA)) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    legend.position = "bottom",
    panel.grid.minor = element_blank() # remove minor grid lines
  )
```

### 4. Conclusions on Total Effect

Based on the logistic regression model results, we can draw the following conclusions regarding the **Total Effect** of comments on trending persistence:

1.  **Statistical Significance (**$p$-value & $z$-score): The calculated $z$-score is **21.59**, which corresponds to a $p$-value of effectively **zero** ($< 2 \times 10^{-16}$). This provides overwhelming evidence to **reject the Null Hypothesis (**$H_0$). We conclude that there is a highly statistically significant relationship between the comment rate and the probability of a video remaining in the trends.

2.  **Direction of Effect (**$c$): The coefficient for comment rate is positive ($c \approx 0.0425$). This confirms that higher viewer engagement (in the form of comments) is associated with a higher probability of the video appearing in the trends on the following day.

Now, in **Part 3**, we will investigate the **mechanism**: Does this happen simply because people comment, or does the discussion drive more *likes*, which in turn signals the algorithm to keep the video trending?

## Part 3. The mediation of likes between comment and "go to next day in trends" probability.

We test whether likes serve as an effective mediator explaining how the number of comments influences a video’s probability of appearing in the trends on the following day.

### 1. Theory and variables for research

In this section, we examine whether the effect of the comment rate\
$$X = comments / views$$\
on the probability that a video reappears in trending the following day\
$$Y = trending\_next\_day$$\
is transmitted indirectly through the likes ratio\
$$M = likes / views.$$

The probability of appearing in trending the next day is defined as a binary event:

$$
Y_i =
\begin{cases}
1, & \text{if video } i \text{ appears again on day } t+1 \\
0, & \text{otherwise}
\end{cases}
$$

and conceptually represents\
$$
\underbrace{P(Y=1)}_{\text{probability to reappear in trending tomorrow}}.
$$

This metric captures *momentum*: a video that stays in trending is favored by YouTube’s algorithm.

#### Hypotheses

We test whether likes act as a statistical mediator:

-   **Null hypothesis**

$$H_0: a \cdot b = 0$$

-   **Alternative hypothesis**

$$H_1: a \cdot b \ne 0$$

Here, $a$ and $b$ are coefficients in the mediation pathway.

#### Model Specification

The mediation structure follows the classical framework:

1.  **Effect of comments on likes** (mediator model):

$$
M = aX + \varepsilon_M
$$

2.  **Effect of comments and likes on trending probability** (outcome model):

$$
logit(P(Y=1)) = c'X + bM + \varepsilon_Y
$$

where

$$
logit(p) = \ln \frac{p}{1 - p}
$$

transforms probabilities (bounded between 0 and 1) into real numbers so that they can be modeled as a linear combination of predictors.

3.  **Total effect of comments** (model without mediator):

$$
logit(P(Y=1)) = cX + \varepsilon.
$$

The coefficients have the following interpretation:

-   $a$: how much comments increase likes\
-   $b$: how much likes increase trending probability\
-   $c$: total effect of comments\
-   $c'$: direct effect of comments after controlling for likes

#### Evaluating the Total Effect of Comments

The causal effects are defined as:

-   **Indirect effect (IE)**:\
    $$
    IE = a \cdot b
    $$

-   **Direct effect (DE)**:\
    $$
    DE = c'
    $$

-   **Total effect (TE)**:\
    $$
    TE = c
    $$

These quantities satisfy the identity:

$$
TE = DE + IE.
$$

This equality **always holds** by construction of the regression models; it is not itself a hypothesis test.

What we test instead is:

> **How much of the total effect** $c$ is transmitted through the mediator?

If $IE$ is large and significant → likes explain part of the effect of comments.

If $IE = 0$ → likes do not mediate anything.

If both $DE$ and $IE$ are significant → partial mediation.

If $DE$ becomes small or non-significant while $IE$ is significant → full mediation.

#### Why We Test Only the Indirect Effect $a \cdot b$

The identity\
$$TE = DE + IE$$\
is always mathematically true, so we do **not** test this equality. Instead, we test whether:

$$
IE = a \cdot b \ne 0.
$$

If $IE$ is non-zero → *likes mediate the effect*.

If $IE = 0$ → *likes do not mediate anything*.

#### Testing the Indirect Effect: Bootstrap Procedure

Because the product $a \cdot b$ does not follow a normal distribution, we assess its significance using bootstrap resampling.

We repeatedly (e.g., 10,000 times):

1.  Sample videos **with replacement**
2.  Refit both regression models to obtain new estimates $a^*$ and $b^*$
3.  Compute the bootstrap indirect effect:

$$
IE^* = a^* \cdot b^*.
$$

The bootstrap-based $p$-value is

$$
p = P(|a \cdot b| \le |a^* \cdot b^*|),
$$

where the probability is taken over the bootstrap distribution.

If the bootstrap confidence interval excludes zero → **significant mediation**.

#### Summary Diagram of Effects

$$
X \longrightarrow M \longrightarrow Y
$$

with:

$$
\begin{aligned}
a &: X \rightarrow M \\
b &: M \rightarrow Y \\
c &: X \rightarrow Y \ (\text{total}) \\
c' &: X \rightarrow Y \ (\text{direct})
\end{aligned}
$$

#### Conclusion

If the indirect effect $a \cdot b$ is statistically significant, then:

> **The likes ratio partially mediates the influence of comment rate on a video's probability of reappearing in trending on the next day.**

Otherwise:

> **Comment activity and likes contribute independently to trending recurrence.**

### 2. Data processing and vizualization

We create a mediation dataset by sampling 150,000 unique videos and adding three key variables: `comment_rate` = $\frac{\text{comments}}{\text{views}} \times 1000$ and `likes_ratio` = $\frac{\text{likes}}{\text{views}} \times 1000$ (scaled by 1000 for numerical stability in logistic regression), plus a binary outcome `trending_next_day` indicating whether the video trended again within 24 hours. After removing NA values from the outcome variable, we obtain a clean dataset ready for mediation analysis.

```{r}
library(tidyverse)
# Here we are taking 150000 unique videos for our dataset
sample_videos <- sample(unique(df$video_id), 150000)

# Filtrating this dataframe
df_mediation <- df %>% filter(video_id %in% sample_videos)

# creating variables for our mediation: comment_rate, likes_ratio, trending_date
df_mediation <- df %>% 
  filter(video_id %in% sample_videos) %>%
  mutate(
    comment_rate = (comment_count / views)*1000,
    likes_ratio = (likes / views)*1000,
    trending_date = ymd_hms(publish_time, tz = "UTC")
  ) %>%
  # Now creating one more variable trending_next_day(0 or 1) - if video goes to trends next day
  arrange(video_id, trending_date) %>%
  group_by(video_id) %>%
  mutate(
    trending_next_day = lead(trending_date, 1) <= (trending_date + days(1)) &
                        lead(trending_date, 1) > trending_date
  ) %>%
  ungroup() %>%
  filter(!is.na(trending_next_day))

df_mediation$trending_next_day <- as.numeric(df_mediation$trending_next_day)
head(df_mediation)

```

**Distribution of Comment Counts** We visualize the distribution of comment counts on a logarithmic scale to handle the wide range of values and identify patterns in engagement.

```{r}
ggplot(df_mediation, aes(x = comment_count)) +
  geom_histogram(bins = 50, fill = "skyblue", color = "black") +
  scale_x_log10() +
  labs(title = "Distribution of Comment Counts",
       x = "Comment Count (log scale)", y = "Frequency")
```

**Top Channels by Videos Trending Next Day & Distribution of Likes** We examine the distribution of likes (log-scaled) and identify the top 10 channels with the most videos that trended again the next day.

```{r}
# Histogram of likes
ggplot(df_mediation, aes(x = likes)) +
  geom_histogram(bins = 50, fill = "salmon", color = "black") +
  scale_x_log10() +
  labs(title = "Distribution of Likes",
       x = "Likes (log scale)", y = "Frequency")

# Top 10 channels by number of next-day trending videos
top_channels <- df_mediation %>%
  filter(trending_next_day == 1) %>%
  group_by(channel_title) %>%
  summarise(n_videos = n()) %>%
  arrange(desc(n_videos)) %>%
  slice(1:10)

ggplot(top_channels, aes(x = reorder(channel_title, n_videos), y = n_videos)) +
  geom_bar(stat="identity", fill="purple") +
  coord_flip() +
  labs(title="Top Channels by Videos Trending Next Day",
       x="Channel", y="Number of Videos")
```

**Dataset Summary & Trending Status** We report the dataset size and calculate the proportion of videos that trended the next day versus those that did not. The dataset contains 180,032 rows despite sampling 150,000 unique videos because each video can appear multiple times in the trending list across different days, and we analyze each trending occurrence separately to predict next-day trending behavior.

```{r}
cat("Number of rows in dataset:", nrow(df_mediation), "\n")

table_trending <- table(df_mediation$trending_next_day)
table_trending
summary(df_mediation)
```

**Comment Rate vs Likes Ratio** We explore the relationship between comment rate and likes ratio through a scatterplot to assess potential correlation between these engagement metrics.

```{r}

ggplot(df_mediation, aes(x=comment_rate, y=likes_ratio)) +
  geom_point(alpha=0.3) +
  labs(title="Likes Ratio vs Comment Rate", x="Comment Rate", y="Likes Ratio")


```

### 3. Finding needed parameters

So we created dataset where we are having for every video his trending_next_day, like_ratio, and comment_ratio.

#### a. Linear Regression model for comments and likes

In this section, we estimate how the mediator **Likes Ratio (M)** depends on the predictor **Comment Rate (X)** using the linear model $M = aX + a_0$. We manually compute the slope $a$ and intercept $a_0$ using the closed-form OLS formulas and then verify the result with `lm()`. This step represents the first stage of the mediation framework, quantifying the effect of $X$ on $M$.

**Symbols:**\
- $X$: comment rate\
- $M$: likes ratio\
- $a$: slope (effect of $X$ on $M$)\
- $a_0$: intercept

```{r}
# Let`s find a
mean_of_comments = mean(df_mediation$comment_rate)
mean_of_likes = mean(df_mediation$likes_ratio)


a = sum((df_mediation$comment_rate - mean_of_comments)*(df_mediation$likes_ratio - mean_of_likes))/sum((df_mediation$comment_rate - mean_of_comments)^2)

a0 = mean_of_likes - a*mean_of_comments

cat("a = ", a, "\n")
cat("a0 = ", a0 , "\n")
lm_fit <- lm(likes_ratio ~ comment_rate, data=df_mediation)

ggplot(df_mediation, aes(x = comment_rate, y = likes_ratio)) +
  geom_point(alpha = 0.3) +
  geom_abline(intercept = a0, slope = a, color = "red", size = 1) +
  labs(
    title = "Likes Ratio vs Comment Rate",
    x = "Comment Rate * 1000",
    y = "Likes Ratio * 1000"
  ) +
  theme_minimal()


summary(lm_fit)

```

The regression summary indicates a strong and highly significant linear association: the estimated slope $a = 1.91$ and intercept $a_0 = 24.10$ both have p-values \< 2e-16, confirming they are statistically different from zero. The model explains approximately 25% of the variance in the likes ratio (Adjusted $R^2 = 0.2503$), showing that comment rate is an important predictor but not the sole driver. The residual standard error of 31.03 suggests moderate dispersion around the fitted line, consistent with the large variability inherent in user-engagement metrics.

#### b. logit regression for comments - likes -\> trending_next_day

This section manually estimates the logistic regression model
$$
logit(P(Y=1)) = c'X + bM + \alpha_0,
$$
where $X$ is the comment rate, $M$ is the likes ratio (mediator), and $Y$ indicates whether the video appears in the trending list the next day.\
We begin by centering all predictors ($X - \bar X$, $M - \bar M$, $Y - \bar Y$) and computing the unweighted cross-product sums $S_{xx}$, $S_{mm}$, $S_{xm}$, $S_{xy}$, and $S_{my}$. These allow us to obtain initial OLS-style estimates:\
$$
b = \frac{S_{xx}S_{my} - S_{xm}S_{xy}}{D}, \qquad
c' = \frac{S_{mm}S_{xy} - S_{xm}S_{my}}{D},
$$

$$
\alpha_0 = \bar Y - c'\bar X - b\bar M,
$$
where $D = S_{xx}S_{mm} - S_{xm}^2$. These serve as starting values for the iterative procedure.

Next, we refine these parameters using **Iteratively Reweighted Least Squares (IRLS)**, the classical algorithm for maximizing the logistic regression log-likelihood. At each iteration we compute the linear predictor
$$
\eta = c'X + bM + \alpha_0,
$$
and transform it into probabilities using the logistic function
$$
p = \frac{1}{1 + e^{-\eta}}.
$$ 
The quantity $w = p(1 - p)$ acts as the local curvature (second derivative) of the log-likelihood and therefore serves as the weight for each observation. The working response variable
$$
u = \eta + \frac{Y - p}{w}
$$ 
is a first-order Taylor approximation of the true log-likelihood around the current parameter values.
Thus, IRLS repeatedly solves a **weighted** least squares system for $(c', b, \alpha_0)$ using weights $w$ and pseudo-response $u$, updating the centered weighted sums and recomputing the coefficients until convergence.

Convergence is reached when the parameter changes
$$
|b - b_{old}|,\quad |c' - c'_{old}|,\quad |\alpha_0 - \alpha_{0,old}|
$$ 
become smaller than a predefined tolerance, meaning the log-likelihood is no longer improving in a meaningful way. At this point, the IRLS solution corresponds to the maximum likelihood estimates of the logistic model.

Finally, the mediation effects are computed as
$$
IE = a \times b,
$$
where $a$ comes from the linear regression $M = aX + a_0$, and
$$
DE = c'.
$$
The obtained coefficients are compared with the standard `glm()` output to validate correctness. This manual IRLS implementation demonstrates how the logistic regression estimator emerges from repeated weighted normal equations and how the curvature of the log-likelihood determines the adaptive weights driving convergence.

```{r}
X <- df_mediation$comment_rate
M <- df_mediation$likes_ratio
Y <- df_mediation$trending_next_day

# Parametrs needed from linear regression

x_bar <- mean(X)
m_bar <- mean(M)
y_bar <- mean(Y)

X_tilde <- X - x_bar
M_tilde <- M - m_bar

S_xx <- sum(X_tilde^2)
S_xm <- sum(X_tilde * M_tilde)


cat("M ~ X\n")
cat("a =", a, "\n")
cat("a0 =", a0, "\n\n")

#logit regression estimation

Y_tilde <- Y - y_bar
S_mm <- sum(M_tilde^2)
S_xy <- sum(X_tilde * Y_tilde)
S_my <- sum(M_tilde * Y_tilde)

D <- S_xx * S_mm - S_xm^2

b <- (S_xx * S_my - S_xm * S_xy) / D
c_prime <- (S_mm * S_xy - S_xm * S_my) / D
intercept_Y <- y_bar - c_prime * x_bar - b * m_bar

# IRLS iterations
epsilon <- 1e-6
max_iter <- 50
iter <- 0

repeat {
  iter <- iter + 1
  
  c_prime_old <- c_prime
  b_old <- b
  intercept_Y_old <- intercept_Y
  
  # predictor and probability
  eta <- c_prime * X + b * M + intercept_Y
  eta <- pmax(pmin(eta, 20), -20)
  p <- 1 / (1 + exp(-eta))
  p <- pmax(pmin(p, 1 - 1e-10), 1e-10)
  
  # Weights and working response
  w <- p * (1 - p)
  w <- pmax(w, 1e-8)
  u <- eta + (Y - p) / w
  
  if (any(is.na(u))) {
    c_prime <- c_prime_old
    b <- b_old
    intercept_Y <- intercept_Y_old
    break
  }
  
  # Weighted avaverages
  w_sum <- sum(w)
  x_bar_w <- sum(w * X) / w_sum
  m_bar_w <- sum(w * M) / w_sum
  u_bar_w <- sum(w * u) / w_sum
  
  X_tilde_w <- X - x_bar_w
  M_tilde_w <- M - m_bar_w
  u_tilde_w <- u - u_bar_w
  
  # Weighted sums
  S_xx_w <- sum(w * X_tilde_w^2)
  S_mm_w <- sum(w * M_tilde_w^2)
  S_xm_w <- sum(w * X_tilde_w * M_tilde_w)
  S_xu_w <- sum(w * X_tilde_w * u_tilde_w)
  S_mu_w <- sum(w * M_tilde_w * u_tilde_w)
  
  if (any(is.na(c(S_xx_w, S_mm_w, S_xm_w, S_xu_w, S_mu_w)))) {
    c_prime <- c_prime_old
    b <- b_old
    intercept_Y <- intercept_Y_old
    break
  }
  
  D_w <- S_xx_w * S_mm_w - S_xm_w^2
  
  if (is.na(D_w) || abs(D_w) < 1e-10) {
    c_prime <- c_prime_old
    b <- b_old
    intercept_Y <- intercept_Y_old
    break
  }
  
  # Updated parametrs
  b <- (S_xx_w * S_mu_w - S_xm_w * S_xu_w) / D_w
  c_prime <- (S_mm_w * S_xu_w - S_xm_w * S_mu_w) / D_w
  intercept_Y <- u_bar_w - c_prime * x_bar_w - b * m_bar_w
  
  # check of convergence 
  delta_b <- abs(b - b_old)
  delta_c <- abs(c_prime - c_prime_old)
  delta_int <- abs(intercept_Y - intercept_Y_old)
  
  if (is.na(delta_b) || is.na(delta_c) || is.na(delta_int)) {
    c_prime <- c_prime_old
    b <- b_old
    intercept_Y <- intercept_Y_old
    break
  }
  
  if (iter >= max_iter) break
  if (delta_b < epsilon && delta_c < epsilon && delta_int < epsilon) break
}

cat("Y ~ X + M (converged at iteration ", iter, ")\n")
cat("c_prime (scaled) =", c_prime, "\n")
cat("b (scaled) =", b, "\n")
cat("intercept_Y =", intercept_Y, "\n\n")

# Back to original scale
c_prime_original <- c_prime * 1000
b_original <- b * 1000

cat("Estimates in the original scale\n")
cat("c_prime =", c_prime_original, "\n")
cat("b =", b_original, "\n")
cat("intercept_Y =", intercept_Y, "\n\n")

# Mediation effects
IE <- (a) * (b)
DE <- c_prime

cat("Mediation Effects\n")
cat("Indirect Effect (a × b) =", IE, "\n")
cat("Direct Effect (c') =", DE, "\n")

# Check against glm
logit_outcome <- glm(Y ~ X + M, family = binomial(link = "logit"))
cat("Comparison with glm()\n")
print(coef(logit_outcome))
cat("\n")

# Statistics of predictions
cat("Prediction Statistics\n")
cat("Min P(Y=1):", min(p_pred), "\n")
cat("Median P(Y=1):", median(p_pred), "\n")
cat("Mean P(Y=1):", mean(p_pred), "\n")
cat("Max P(Y=1):", max(p_pred), "\n")
cat("Share of Y=1 in data:", mean(Y), "\n")
```
The estimated linear model for the mediator gives $M = aX + a_0$ with  
$a = 2.021576$ and $a_0 = 24.53683$, indicating that each unit increase in the comment rate is associated with an average increase of approximately $2.02$ units in the likes ratio.  
In the logistic regression stage, the IRLS algorithm converged after 8 iterations, producing the scaled coefficients $c' = 0.006710927$ and $b = -0.003358462$, where "scaled" refers to the fact that $X$ and $M$ were internally rescaled by $1000$ to improve numerical stability of the iterations. After returning to the original units of measurement, these values become $c' = 6.710927$ and $b = -3.358462$, while the intercept remains unchanged at $-5.094691$ because it already reflects the original data scale.

The indirect mediation effect equals $IE = a \times b = -0.006789385$, revealing that the pathway $X \rightarrow M \rightarrow Y$ slightly suppresses the overall effect, since the product $a \times b$ is negative. The direct effect is $DE = c' = 0.006710927$ (in scaled form), indicating that increases in comment rate directly raise the log-odds of trending, independent of likes. The agreement between the manually estimated coefficients and the `glm()` output confirms the correctness of the IRLS procedure, as the numbers match exactly: $(\alpha_0 = -5.09469$, $c' = 0.006710927$, $b = -0.003358462)$.

Finally, the prediction statistics show that typical predicted probabilities of trending are extremely small: the median and mean probabilities are both about $0.0057$, while the maximum predicted value is only $0.1056$. This aligns with the empirical share of trending videos in the dataset ($0.00564$), demonstrating that the fitted model captures the strong class imbalance and produces probabilities consistent with real-world frequencies.

#### c. Visualization of the Mediation Model

The following visualizations provide an intuitive overview of the relationships captured by the mediation model.  
We examine model-predicted trending probabilities across the range of comment rates and compare the model’s trend with the empirical bin-averaged frequencies.  
A histogram of predicted probabilities (in log-scale) illustrates the extreme class imbalance and how the model distributes probability mass.  
Finally, a calibration plot compares predicted vs. observed frequencies to assess how well the logistic model aligns with empirical outcomes.

```{r}
# Predicted trending probability vs Comment Rate

eta_pred <- c_prime_original * (X/1000) + b_original * (M/1000) + intercept_Y
p_pred <- 1 / (1 + exp(-eta_pred))

# Binning
x_bins <- cut(X/1000, breaks = 20, include.lowest = TRUE)

x_bin_mid <- tapply(X/1000, x_bins, function(v) mean(v, na.rm = TRUE))
p_bin_mean <- tapply(p_pred, x_bins, function(v) mean(v, na.rm = TRUE))
y_bin_obs  <- tapply(Y, x_bins, function(v) mean(v, na.rm = TRUE))

# Convert to numeric
x_bin_mid  <- as.numeric(x_bin_mid)
p_bin_mean <- as.numeric(p_bin_mean)
y_bin_obs  <- as.numeric(y_bin_obs)

# Main plot
plot(X/1000, p_pred,
     pch = 16, cex = 0.3, col = rgb(0, 0, 1, 0.15),
     xlab = "Comment Rate",
     ylab = "P(Y=1)",
     main = "Predicted Probability of Trending",
     ylim = c(0, max(c(p_bin_mean, y_bin_obs), na.rm = TRUE) * 1.2)
)

# Filter bins with data
valid <- complete.cases(x_bin_mid, p_bin_mean)
x_plot <- x_bin_mid[valid]
p_plot <- p_bin_mean[valid]

# Draw model trend line
lines(sort(x_plot), p_plot[order(x_plot)], col = "blue", lwd = 3)

# Observed frequencies
points(x_plot, y_bin_obs[valid], col = "red", pch = 19, cex = 1.3)

legend(
  "topleft",
  legend = c("Model Prediction (Avg in bins)", "Observed Trending Rate"),
  col = c("blue", "red"),
  lwd = c(3, NA),
  pch = c(NA, 19),
  bty = "n"
)


# Distribution of predicted probabilities (log-scale)

hist(log10(p_pred + 1e-10), breaks = 50, col = "lightblue", border = "white",
     xlab = "log10(P(Y=1))", main = "Distribution of Predicted Probabilities (Log Scale)")
abline(v = log10(mean(p_pred)), col = "red", lwd = 2, lty = 2)


# Calibration Plot

p_bins <- cut(p_pred, breaks = quantile(p_pred, probs = seq(0, 1, 0.1)), include.lowest = TRUE)
obs_freq <- tapply(Y, p_bins, mean)
pred_freq <- tapply(p_pred, p_bins, mean)

plot(pred_freq, obs_freq, pch = 19, cex = 1.5, col = "darkgreen",
     xlab = "Predicted P(Y=1)", ylab = "Observed Frequency of Y=1",
     main = "Model Calibration",
     xlim = c(0, max(pred_freq)*1.1),
     ylim = c(0, max(obs_freq, na.rm = TRUE)*1.1))

abline(0, 1, lty = 2, col = "gray", lwd = 2)
grid()
```

### 4. p-value and Testing (Bootstrap Test for the Indirect Effect *a·b*)

To evaluate whether the mediation effect is statistically significant, we test the indirect pathway  
$$
a \cdot b
$$
using a **bootstrap procedure**. Bootstrapping is preferred because the sampling distribution of the product of coefficients is often non-normal, making traditional analytic tests inaccurate.  

Below, we resample the dataset with replacement, refit both mediator and outcome models for each bootstrap sample, and compute the product $\hat{a}^\*(b^\*)$ for every iteration.  
From this bootstrap distribution, we then compute:

- the bootstrap standard error  
- the 95% confidence interval  
- the p-value using the percentile method  

Finally, we compare the observed indirect effect  
$$
\widehat{ab} = a_{orig} \cdot b_{orig}
$$
to the bootstrap distribution.

**For unbalanced sample**

In this part of the analysis, we estimate the indirect effect *a·b* using a nonparametric bootstrap applied to a large random subsample of the original, unbalanced dataset. The purpose of using bootstrap resampling is to approximate the sampling distribution of the mediation parameters by repeatedly generating new datasets through sampling with replacement. Because both components of the indirect path depend on fitted models, we re-estimate the linear regression $M \sim X$ and the logistic regression $Y \sim X + M$ in every bootstrap iteration rather than reusing the original coefficients. This re-fitting step ensures that each bootstrap sample produces its own estimates $\hat{a}$ and $\hat{b}$, capturing the true variability of the mediation effect under repeated sampling. Finally, by comparing the observed indirect effect with the empirical bootstrap distribution, we obtain a percentile confidence interval and a p-value that quantify whether the observed effect is larger than would be expected from sampling variability alone.

```{r}
set.seed(122)
B <- 1000               # number of bootstrap samples
n_sub <- 5000           # size of subsample to speed up computation

ab_boot <- numeric(B)   # to store bootstrap products

n <- length(X)

# create a random subsample of indices
sub_idx <- sample(1:n, n_sub, replace = FALSE)
X_sub <- X[sub_idx]
M_sub <- M[sub_idx]
Y_sub <- Y[sub_idx]

for (i in 1:B) {
  
  # draw bootstrap sample indices within the subsample
  idx <- sample(1:n_sub, size = n_sub, replace = TRUE)

  Xb <- X_sub[idx]
  Mb <- M_sub[idx]
  Yb <- Y_sub[idx]

  # refit mediator model M ~ X
  med_fit <- lm(Mb ~ Xb)
  a_boot  <- coef(med_fit)[2]

  # refit outcome model Y ~ X + M (logistic)
  out_fit <- glm(Yb ~ Xb + Mb, family = binomial)
  b_boot  <- coef(out_fit)[3]

  # store product
  ab_boot[i] <- a_boot * b_boot
}

# Observed indirect effect
a_original <- a
ab_obs <- a_original * b_original

# Bootstrap statistics
ci_low  <- quantile(ab_boot, 0.025)
ci_high <- quantile(ab_boot, 0.975)

# Two-sided percentile p-value
p_value <- mean(abs(ab_boot) >= abs(ab_obs))

# Output
cat("Observed indirect effect (a*b): ", ab_obs, "\n")
cat("Bootstrap 95% CI: [", ci_low, ", ", ci_high, "]\n")
cat("Bootstrap p-value: ", p_value, "\n\n")
```
The code above performs a bootstrap test of the indirect effect $a \times b$ in a mediation framework, using a computationally efficient subsampling strategy.  
In each bootstrap iteration, we resample a subset of the data with replacement, refit both the mediator model $ M \sim X $ and the outcome model $Y \sim X + M$, and compute the product of the estimated coefficients to obtain a bootstrap estimate of the indirect effect.  
This procedure produces an empirical sampling distribution of $a \times b$, from which we derive the percentile-based 95% confidence interval and the two-sided bootstrap p-value.  
In our results, the observed indirect effect is $-6.760537$, while the bootstrap confidence interval $[-0.03756072,\, 0.02001724]$ contains values close to zero, indicating that the indirect effect is extremely small relative to the variability of the estimator.  
The bootstrap p-value equals **0**, meaning that none of the 1000 bootstrap samples produced an effect as extreme as the observed one, which suggests that the estimated indirect effect is statistically distinguishable from the null under this subsampled bootstrap procedure, although such a result should be interpreted cautiously due to the strong class imbalance and model instability in the data.

**For balanced sample**
To correct the strong class imbalance in the original dataset, we construct a balanced sample by identifying the indices of all positive cases (idx_1) and all negative cases (idx_0).  
We then draw an equal number of observations from each class, using n_min as the shared sample size and sampling without replacement to avoid duplication.  
By combining the selected indices into idx_bal_fixed, we form balanced vectors X_bal, M_bal, and Y_bal that contain an equal proportion of Y = 0 and Y = 1 cases.  
This balanced dataset allows both the mediator model M_bal ~ X_bal and the logistic outcome model Y_bal ~ X_bal + M_bal to estimate coefficients that are not dominated by the majority class.  
As a result, the indirect effect a_bal_fixed × b_bal_fixed and its bootstrap distribution reflect genuine signal rather than class-frequency artifacts, making inference more stable and interpretable.

```{r}
library(MASS)

set.seed(123)
B_bal <- 2000   # number of bootstrap samples

# separate classes
idx_1 <- which(Y == 1)
idx_0 <- which(Y == 0)

# fixed balanced sample (used for metrics)
n_min <- min(length(idx_1), length(idx_0))
idx0_fixed <- sample(idx_0, n_min, replace = FALSE)
idx1_fixed <- sample(idx_1, n_min, replace = FALSE)
idx_bal_fixed <- c(idx0_fixed, idx1_fixed)

X_bal <- X[idx_bal_fixed]
M_bal <- M[idx_bal_fixed]
Y_bal <- Y[idx_bal_fixed]

# Observed values for a and b on balanced set
med_fit_fixed <- lm(M_bal ~ X_bal)
a_bal_fixed <- coef(med_fit_fixed)[2]

out_fit_fixed <- glm(Y_bal ~ X_bal + M_bal, family = binomial)
b_bal_fixed <- coef(out_fit_fixed)[3]

ab_obs_bal <- a_bal_fixed * b_bal_fixed

# Bootstrap
ab_boot_bal <- numeric(B_bal)
n_bal <- length(X_bal)

for (i in 1:B_bal) {
  idx <- sample(1:n_bal, size = n_bal, replace = TRUE)

  med_fit_b <- lm(M_bal[idx] ~ X_bal[idx])
  a_b <- coef(med_fit_b)[2]

  out_fit_b <- glm(Y_bal[idx] ~ X_bal[idx] + M_bal[idx], family = binomial)
  b_b <- coef(out_fit_b)[3]

  ab_boot_bal[i] <- a_b * b_b
}

# Bootstrap statistics
ci_low_bal  <- quantile(ab_boot_bal, 0.025)
ci_high_bal <- quantile(ab_boot_bal, 0.975)
p_value_bal <- mean(abs(ab_boot_bal) >= abs(ab_obs_bal))

cat("Balanced sample: Observed indirect effect (a*b): ", ab_obs_bal, "\n")
cat("Bootstrap 95% CI: [", ci_low_bal, ", ", ci_high_bal, "]\n")
cat("Bootstrap p-value: ", p_value_bal, "\n")
```
The observed indirect effect a*b in the balanced dataset equals −0.00694, indicating that the mediated influence of X on Y through M is extremely small in magnitude.  
The bootstrap confidence interval from −0.01689 to 0.0000127 contains zero, showing that the empirical distribution of bootstrap estimates does not support a reliably non-zero indirect effect.  
A bootstrap p-value of 0.487 means that nearly half of the bootstrap samples produced an indirect effect larger in absolute value than the observed one, implying that the observed estimate is not statistically distinguishable from noise.  
Because both a_bal_fixed and b_bal_fixed were estimated on a dataset with equal numbers of Y = 0 and Y = 1 cases, this result suggests that the absence of significance is not caused by class imbalance but by a genuinely weak mediated relationship.  
Therefore, the balanced-data bootstrap analysis supports the conclusion that mediation through M is negligible, and any effect of X on Y is likely direct or too weak to be reliably detected with the current model.
### 5. Conclusion
The mediation analysis of our dataset reveals that the indirect effect of comment rate ($X$) on next-day trending probability ($Y$) through likes ratio ($M$) is extremely small. Using both the unbalanced and balanced bootstrap procedures, we observe that the estimated indirect effect $a \cdot b$ is close to zero:

Unbalanced sample: $IE = -6.76 \times 10^{-3}$, 95% CI $[-0.0376, 0.0200]$, bootstrap p-value = 0

Balanced sample: $IE = -6.94 \times 10^{-3}$, 95% CI $[-0.0169, 0.000013]$, bootstrap p-value = 0.487

These results indicate that the mediated pathway $X \rightarrow M \rightarrow Y$ is not statistically significant once we account for class imbalance and the variability inherent in the data. In particular, the balanced-sample analysis demonstrates that even with equal representation of trending and non-trending cases, the indirect effect remains negligible, and the null hypothesis $H_0: a \cdot b = 0$ cannot be rejected.

The direct effect $c'$, representing the influence of comment rate on trending probability independent of likes, remains positive but small, consistent with the low overall probability of videos reappearing in trending (median predicted $P(Y=1) \approx 0.0057$). The IRLS logistic regression captures the extreme class imbalance, but predictions are largely constrained to the low probability range, reflecting limited predictive power of engagement metrics for next-day trending.

Overall, our analysis suggests that while comment rate slightly affects likes ratio, this pathway does not meaningfully mediate the probability of a video trending again the following day. The model’s limited accuracy and low explained variance imply that additional factors—such as video content, publishing time, or external promotion—likely drive momentum in trending behavior beyond the engagement metrics considered here.
